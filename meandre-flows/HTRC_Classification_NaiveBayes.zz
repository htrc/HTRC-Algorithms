#
# Generated by RDF2ZZConverter v1.4.11.8693M on Mon Mar 23 19:57:28 CDT 2015
#
# @name 	HTRC_Classification_NaiveBayes
# @description 	
# @creator 	admin
# @date 	Mon Mar 23 19:56:54 CDT 2015
# @rights 	UIUC/NCSA Open Source License
# @tags 	naive bayes, classification, htrc
# @uri  	meandre://htrc.illinois.edu/flows/htrc_classification_naivebayes/
#

#
# Specify component imports
#
# TODO: Add component import statement(s) here
# Example: import <URL>   (replace 'URL' with the correct location)
import <http://dev5.informatics.illinois.edu:1714/public/services/repository.ttl>
#
# Create the component aliases
#
alias <meandre://seasr.org/components/foundry/tuple-value-frequency-counter> as TUPLE_VALUE_FREQUENCY_COUNTER
alias <meandre://seasr.org/components/foundry/fork-x2> as FORK_X2
alias <meandre://seasr.org/components/data-mining/model-predict> as MODEL_PREDICT
alias <meandre://seasr.org/components/foundry/opennlp-sentence-detector> as OPENNLP_SENTENCE_DETECTOR
alias <meandre://seasr.org/components/foundry/csv-to-tuple> as CSV_TO_TUPLE
alias <meandre://seasr.org/components/foundry/opennlp-pos-tagger> as OPENNLP_POS_TAGGER
alias <meandre://seasr.org/components/foundry/write-to-file> as WRITE_TO_FILE
alias <meandre://seasr.org/components/foundry/text-replacement> as TEXT_REPLACEMENT
alias <meandre://seasr.org/components/foundry/text-cleaner> as TEXT_CLEANER
alias <meandre://seasr.org/components/data-mining/autobin> as AUTOBIN
alias <meandre://seasr.org/components/foundry/trigger-message-stream-passthrough> as TRIGGER_MESSAGE_STREAM_PASSTHROUGH
alias <meandre://seasr.org/components/foundry/token-count-to-feature-table> as TOKEN_COUNT_TO_FEATURE_TABLE
alias <meandre://seasr.org/components/foundry/tuple-labeler> as TUPLE_LABELER
alias <meandre://seasr.org/components/foundry/read-text> as READ_TEXT
alias <meandre://seasr.org/components/foundry/csv-to-token-counts> as CSV_TO_TOKEN_COUNTS
alias <meandre://seasr.org/components/data-mining/prediction-report-table> as PREDICTION_REPORT_TABLE
alias <meandre://seasr.org/components/data-mining/createnbmodel> as CREATENBMODEL
alias <meandre://seasr.org/components/foundry/fork-x5> as FORK_X5
alias <meandre://seasr.org/components/data-mining/simpletraintest> as SIMPLETRAINTEST
alias <meandre://seasr.org/components/foundry/universal-text-extractor> as UNIVERSAL_TEXT_EXTRACTOR
alias <meandre://seasr.org/components/foundry/tuple-to-csv> as TUPLE_TO_CSV
alias <meandre://seasr.org/components/foundry/tuple-value-to-string> as TUPLE_VALUE_TO_STRING
alias <meandre://seasr.org/components/foundry/table-to-csv> as TABLE_TO_CSV
alias <meandre://seasr.org/components/foundry/search-text> as SEARCH_TEXT
alias <meandre://seasr.org/components/data-mining/createbintree> as CREATEBINTREE
alias <meandre://seasr.org/components/foundry/strings-to-java-string> as STRINGS_TO_JAVA_STRING
alias <meandre://seasr.org/components/foundry/text-accumulator> as TEXT_ACCUMULATOR
alias <meandre://seasr.org/components/data-mining/featurefilterlite> as FEATUREFILTERLITE
alias <meandre://seasr.org/components/data-mining/confusion-matrix> as CONFUSION_MATRIX
alias <meandre://seasr.org/components/foundry/push-text> as PUSH_TEXT
alias <meandre://seasr.org/components/foundry/random-sample> as RANDOM_SAMPLE
alias <meandre://seasr.org/components/foundry/flow-parameter> as FLOW_PARAMETER
alias <meandre://seasr.org/components/htrc/htrc-page-retriever> as HTRC_PAGE_RETRIEVER
alias <meandre://seasr.org/components/foundry/opennlp-sentence-tokenizer> as OPENNLP_SENTENCE_TOKENIZER
alias <meandre://seasr.org/components/foundry/stream-delimiter-filter> as STREAM_DELIMITER_FILTER
alias <meandre://seasr.org/components/foundry/trigger-message> as TRIGGER_MESSAGE

#
# Create the component instances
#
tuple_value_frequency_counter = TUPLE_VALUE_FREQUENCY_COUNTER()
autobin = AUTOBIN()
write_to_file = WRITE_TO_FILE()
confusion_matrix = CONFUSION_MATRIX()
text_accumulator = TEXT_ACCUMULATOR()
prediction_report_table = PREDICTION_REPORT_TABLE()
csv_to_tuple = CSV_TO_TUPLE()
push_text = PUSH_TEXT()
tuple_value_to_string = TUPLE_VALUE_TO_STRING()
stream_filter = STREAM_DELIMITER_FILTER()
stream_filter_2 = STREAM_DELIMITER_FILTER()
csv_to_token_counts = CSV_TO_TOKEN_COUNTS()
token_count_to_feature_table = TOKEN_COUNT_TO_FEATURE_TABLE()
createnbmodel = CREATENBMODEL()
text_replacement = TEXT_REPLACEMENT()
search_text = SEARCH_TEXT()
featurefilterlite = FEATUREFILTERLITE()
table_to_csv = TABLE_TO_CSV()
trigger_message = TRIGGER_MESSAGE()
sentence_tokenizer = OPENNLP_SENTENCE_TOKENIZER()
simpletraintest = SIMPLETRAINTEST()
universal_text_extractor_2 = UNIVERSAL_TEXT_EXTRACTOR()
fork_x5 = FORK_X5()
model_predict = MODEL_PREDICT()
write_to_file_2 = WRITE_TO_FILE()
trigger_message_2 = TRIGGER_MESSAGE()
prediction_report_table_2 = PREDICTION_REPORT_TABLE()
read_text = READ_TEXT()
createbintree = CREATEBINTREE()
tuple_to_csv = TUPLE_TO_CSV()
csv_to_tuple_2 = CSV_TO_TUPLE()
confusion_matrix_2 = CONFUSION_MATRIX()
tuple_labeler = TUPLE_LABELER()
table_to_csv_2 = TABLE_TO_CSV()
strings_to_java_string = STRINGS_TO_JAVA_STRING()
text_accumulator_2 = TEXT_ACCUMULATOR()
table_to_csv_3 = TABLE_TO_CSV()
model_predict_2 = MODEL_PREDICT()
random_sample = RANDOM_SAMPLE()
write_to_file_3 = WRITE_TO_FILE()
sentence_detector = OPENNLP_SENTENCE_DETECTOR()
csv_to_tuple_3 = CSV_TO_TUPLE()
htrc_page_retriever = HTRC_PAGE_RETRIEVER()
push_text_3 = PUSH_TEXT()
search_text_2 = SEARCH_TEXT()
trigger_stream_passthrough = TRIGGER_MESSAGE_STREAM_PASSTHROUGH()
text_cleaner_2 = TEXT_CLEANER()
fork_x2 = FORK_X2()
push_text_2 = PUSH_TEXT()
fork_x2_2 = FORK_X2()
flow_parameter = FLOW_PARAMETER()
table_to_csv_4 = TABLE_TO_CSV()
tuple_value_to_string_2 = TUPLE_VALUE_TO_STRING()
fork_x2_3 = FORK_X2()
fork_x2_4 = FORK_X2()
write_to_file_4 = WRITE_TO_FILE()
pos_tagger = OPENNLP_POS_TAGGER()
push_text_4 = PUSH_TEXT()
stream_filter_3 = STREAM_DELIMITER_FILTER()
fork_x2_5 = FORK_X2()
flow_parameter_2 = FLOW_PARAMETER()
text_cleaner = TEXT_CLEANER()

#
# Set component properties
#
tuple_value_frequency_counter.tupleField = "token"
tuple_value_frequency_counter._ignore_errors = "false"
tuple_value_frequency_counter.trim_fields = "token"
tuple_value_frequency_counter.threshold = "2"
tuple_value_frequency_counter.max_size = "100"
tuple_value_frequency_counter.normalize_fields = "token"
tuple_value_frequency_counter._debug_level = "info"

autobin._debug_level = "info"
autobin.nrOfBins = "2"
autobin.weight = "1"
autobin._ignore_errors = "false"
autobin.method = "0"

write_to_file._debug_level = "info"
write_to_file.append_timestamp = "false"
write_to_file.append_data_to_file = "false"
write_to_file._ignore_errors = "false"
write_to_file.default_folder = "/tmp"

confusion_matrix._debug_level = "info"
confusion_matrix._stream_id = "0"
confusion_matrix._ignore_errors = "false"
confusion_matrix.wrap_stream = "false"

text_accumulator._debug_level = "info"
text_accumulator.separator = " "
text_accumulator._stream_id = "2"
text_accumulator._ignore_errors = "false"

prediction_report_table._debug_level = "info"
prediction_report_table._ignore_errors = "false"

csv_to_tuple._debug_level = "info"
csv_to_tuple.header = "false"
csv_to_tuple.labels = "volume_id"
csv_to_tuple._ignore_errors = "false"

push_text._debug_level = "info"
push_text.message = "test_confusion_matrix.csv"
push_text._ignore_errors = "false"

tuple_value_to_string._debug_level = "info"
tuple_value_to_string.attribute = "volume_id"
tuple_value_to_string._ignore_errors = "false"

stream_filter._debug_level = "info"
stream_filter.advanced_filter = ""
stream_filter._stream_id = ""
stream_filter._ignore_errors = "false"

stream_filter_2._debug_level = "info"
stream_filter_2.advanced_filter = ""
stream_filter_2._stream_id = "2"
stream_filter_2._ignore_errors = "false"

csv_to_token_counts.enable_constraint_checking = "false"
csv_to_token_counts.column_count = "2"
csv_to_token_counts._ignore_errors = "false"
csv_to_token_counts.header = "true"
csv_to_token_counts.ordered = "true"
csv_to_token_counts.token_pos = "0"
csv_to_token_counts.count_pos = "1"
csv_to_token_counts._debug_level = "info"

token_count_to_feature_table._debug_level = "info"
token_count_to_feature_table.class_column_label = "_class"
token_count_to_feature_table._stream_id = "1"
token_count_to_feature_table._ignore_errors = "false"

createnbmodel._debug_level = "info"
createnbmodel._ignore_errors = "false"

text_replacement.ignoreCase = "true"
text_replacement._debug_level = "info"
text_replacement._ignore_errors = "false"

search_text._debug_level = "info"
search_text._stream_id = "1"
search_text.expression = "(?:[^|]+\\|?){1,4}"
search_text._ignore_errors = "false"
search_text.wrap_stream = "true"

featurefilterlite.removeColumnsWithAllEntries = "true"
featurefilterlite.verbose = "false"
featurefilterlite._ignore_errors = "false"
featurefilterlite.removeColumnsWithOnlyOneEntry = "true"
featurefilterlite.upperBoundSupport = "100"
featurefilterlite.lowerBoundSupport = "0"
featurefilterlite._debug_level = "info"

table_to_csv._debug_level = "info"
table_to_csv._ignore_errors = "false"
table_to_csv.header = "true"

trigger_message.reset_on_push = "false"
trigger_message._debug_level = "info"
trigger_message._stream_id = "1"
trigger_message._ignore_errors = "false"

sentence_tokenizer._debug_level = "info"
sentence_tokenizer.lang_code = "en"
sentence_tokenizer._ignore_errors = "false"

simpletraintest.samplingMethod = "1"
simpletraintest.testPercent = "40"
simpletraintest.verbose = "true"
simpletraintest._ignore_errors = "false"
simpletraintest.seed = "123"
simpletraintest.trainPercent = "60"
simpletraintest._debug_level = "info"

universal_text_extractor_2._debug_level = "info"
universal_text_extractor_2.connection_timeout = "0"
universal_text_extractor_2.read_timeout = "0"
universal_text_extractor_2._ignore_errors = "false"

fork_x5.replication_mode = "2"
fork_x5._debug_level = "info"
fork_x5.replication_method_name = ""
fork_x5._ignore_errors = "false"

model_predict._debug_level = "info"
model_predict._ignore_errors = "false"

write_to_file_2._debug_level = "info"
write_to_file_2.append_timestamp = "false"
write_to_file_2.append_data_to_file = "false"
write_to_file_2._ignore_errors = "false"
write_to_file_2.default_folder = "/tmp"

trigger_message_2.reset_on_push = "false"
trigger_message_2._debug_level = "info"
trigger_message_2._stream_id = "-1"
trigger_message_2._ignore_errors = "false"

prediction_report_table_2._debug_level = "info"
prediction_report_table_2._ignore_errors = "false"

read_text.retry_on_timeout = "true"
read_text.read_timeout = "0"
read_text.max_attempts = "1"
read_text._ignore_errors = "false"
read_text.retry_on_http_error = "0"
read_text.retry_delay = "1000"
read_text.connection_timeout = "0"
read_text._debug_level = "info"

createbintree._debug_level = "info"
createbintree._ignore_errors = "false"

tuple_to_csv._debug_level = "info"
tuple_to_csv._ignore_errors = "false"
tuple_to_csv.header = "true"

csv_to_tuple_2._debug_level = "info"
csv_to_tuple_2._ignore_errors = "false"
csv_to_tuple_2.header = "true"
csv_to_tuple_2.labels = ""

confusion_matrix_2._debug_level = "info"
confusion_matrix_2._stream_id = "0"
confusion_matrix_2.wrap_stream = "false"
confusion_matrix_2._ignore_errors = "false"

tuple_labeler._debug_level = "info"
tuple_labeler.key = "volume_id"
tuple_labeler.hashKey = "volume_id"
tuple_labeler.hashValue = "class"
tuple_labeler._ignore_errors = "false"

table_to_csv_2._debug_level = "info"
table_to_csv_2._ignore_errors = "false"
table_to_csv_2.header = "true"

strings_to_java_string._debug_level = "info"
strings_to_java_string.separator = "\\n"
strings_to_java_string._ignore_errors = "false"

text_accumulator_2._debug_level = "info"
text_accumulator_2.separator = " "
text_accumulator_2._stream_id = "2"
text_accumulator_2._ignore_errors = "false"

table_to_csv_3._debug_level = "info"
table_to_csv_3.header = "true"
table_to_csv_3._ignore_errors = "false"

model_predict_2._debug_level = "info"
model_predict_2._ignore_errors = "false"

random_sample.seed = "123"
random_sample._debug_level = "info"
random_sample.count = "10"
random_sample._ignore_errors = "false"

write_to_file_3._debug_level = "info"
write_to_file_3.append_timestamp = "false"
write_to_file_3.append_data_to_file = "false"
write_to_file_3._ignore_errors = "false"
write_to_file_3.default_folder = "/tmp"

sentence_detector._debug_level = "info"
sentence_detector.lang_code = "en"
sentence_detector.remove_newline = "true"
sentence_detector._ignore_errors = "false"

csv_to_tuple_3._debug_level = "info"
csv_to_tuple_3._ignore_errors = "false"
csv_to_tuple_3.header = "true"
csv_to_tuple_3.labels = ""

htrc_page_retriever.delimiter = "|"
htrc_page_retriever.wrap_stream = "true"
htrc_page_retriever.read_timeout = "0"
htrc_page_retriever.stream_per_volume = "true"
htrc_page_retriever._ignore_errors = "false"
htrc_page_retriever.auth_selfsign = "false"
htrc_page_retriever.auth_token = "648e82d9ca7344813cd5bba78a5b056"
htrc_page_retriever._stream_id = "2"
htrc_page_retriever.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever.connection_timeout = "0"
htrc_page_retriever._debug_level = "info"

push_text_3._debug_level = "info"
push_text_3.message = "train_confusion_matrix.csv"
push_text_3._ignore_errors = "false"

search_text_2._debug_level = "info"
search_text_2.expression = "^\\S*"
search_text_2._stream_id = "0"
search_text_2.wrap_stream = "false"
search_text_2._ignore_errors = "false"

trigger_stream_passthrough.reset_on_push = "false"
trigger_stream_passthrough._debug_level = "info"
trigger_stream_passthrough.passthrough_port = "trigger"
trigger_stream_passthrough.output_stream_port = "object,trigger"
trigger_stream_passthrough._ignore_errors = "false"

text_cleaner_2.replace4 = ""
text_cleaner_2.replace3 = ""
text_cleaner_2.replace2 = ""
text_cleaner_2.replace = "|"
text_cleaner_2._ignore_errors = "false"
text_cleaner_2.find = "\\n"
text_cleaner_2.find4 = ""
text_cleaner_2.find3 = ""
text_cleaner_2.find2 = ""
text_cleaner_2._debug_level = "info"

fork_x2.replication_mode = "2"
fork_x2._debug_level = "info"
fork_x2.replication_method_name = ""
fork_x2._ignore_errors = "false"

push_text_2._debug_level = "info"
push_text_2.message = "test_prediction.csv"
push_text_2._ignore_errors = "false"

fork_x2_2.replication_mode = "0"
fork_x2_2.replication_method_name = ""
fork_x2_2._debug_level = "info"
fork_x2_2._ignore_errors = "false"

flow_parameter.default_value = "/home/lauvil/HTRC/data/sandbox_volume_id_classify_labelled.txt"
flow_parameter._debug_level = "info"
flow_parameter.param_name = "volume_id_labelled"
flow_parameter._stream_id = "0"
flow_parameter._ignore_errors = "false"
flow_parameter.wrap_stream = "false"

table_to_csv_4._debug_level = "info"
table_to_csv_4.header = "true"
table_to_csv_4._ignore_errors = "false"

tuple_value_to_string_2._debug_level = "info"
tuple_value_to_string_2.attribute = "class"
tuple_value_to_string_2._ignore_errors = "false"

fork_x2_3.replication_mode = "2"
fork_x2_3.replication_method_name = ""
fork_x2_3._debug_level = "info"
fork_x2_3._ignore_errors = "false"

fork_x2_4.replication_mode = "0"
fork_x2_4._debug_level = "info"
fork_x2_4.replication_method_name = ""
fork_x2_4._ignore_errors = "false"

write_to_file_4._debug_level = "info"
write_to_file_4.append_timestamp = "false"
write_to_file_4.append_data_to_file = "false"
write_to_file_4.default_folder = "/tmp"
write_to_file_4._ignore_errors = "false"

pos_tagger._debug_level = "info"
pos_tagger.lang_code = "en"
pos_tagger.filter_regex = "NN|NNS|JJ|JJR|JJS|RB|RBR|RP|VB|VBD|VBG|VBN|VBP|VBZ"
pos_tagger._ignore_errors = "false"

push_text_4._debug_level = "info"
push_text_4.message = "train_prediction.csv"
push_text_4._ignore_errors = "false"

stream_filter_3.advanced_filter = ""
stream_filter_3._debug_level = "info"
stream_filter_3._stream_id = "2"
stream_filter_3._ignore_errors = "false"

fork_x2_5.replication_mode = "2"
fork_x2_5.replication_method_name = ""
fork_x2_5._debug_level = "info"
fork_x2_5._ignore_errors = "false"

flow_parameter_2.default_value = "http://repository.seasr.org/Datasets/Text/ngram_corrections.txt"
flow_parameter_2._debug_level = "info"
flow_parameter_2.param_name = "replacement_rules_url"
flow_parameter_2._stream_id = "99"
flow_parameter_2.wrap_stream = "false"
flow_parameter_2._ignore_errors = "false"

text_cleaner.replace4 = ""
text_cleaner.replace3 = ""
text_cleaner.replace2 = ""
text_cleaner.replace = ""
text_cleaner._ignore_errors = "false"
text_cleaner.find = "^(.*)\\s"
text_cleaner.find4 = ""
text_cleaner.find3 = "(?m)--?\\s*$\\s*"
text_cleaner.find2 = "(.*)$"
text_cleaner._debug_level = "info"

#
# Create the flow by connecting the components
#
@autobin_outputs = autobin()
@model_predict_2_outputs = model_predict_2()
@trigger_message_2_outputs = trigger_message_2()
@csv_to_token_counts_outputs = csv_to_token_counts()
@featurefilterlite_outputs = featurefilterlite()
@read_text_outputs = read_text()
@text_replacement_outputs = text_replacement()
@pos_tagger_outputs = pos_tagger() [+AUTO!]
@tuple_value_to_string_outputs = tuple_value_to_string()
@push_text_outputs = push_text()
@confusion_matrix_outputs = confusion_matrix()
@tuple_to_csv_outputs = tuple_to_csv()
@sentence_tokenizer_outputs = sentence_tokenizer() [+AUTO!]
@confusion_matrix_2_outputs = confusion_matrix_2()
@csv_to_tuple_3_outputs = csv_to_tuple_3()
@csv_to_tuple_2_outputs = csv_to_tuple_2()
@text_cleaner_2_outputs = text_cleaner_2()
@token_count_to_feature_table_outputs = token_count_to_feature_table()
@fork_x2_5_outputs = fork_x2_5()
@fork_x2_4_outputs = fork_x2_4()
@fork_x2_3_outputs = fork_x2_3()
@fork_x2_2_outputs = fork_x2_2()
@search_text_outputs = search_text()
@fork_x5_outputs = fork_x5()
@flow_parameter_2_outputs = flow_parameter_2()
@random_sample_outputs = random_sample()
@fork_x2_outputs = fork_x2()
@model_predict_outputs = model_predict()
@tuple_labeler_outputs = tuple_labeler()
@universal_text_extractor_2_outputs = universal_text_extractor_2()
@sentence_detector_outputs = sentence_detector() [+AUTO!]
@prediction_report_table_outputs = prediction_report_table()
@strings_to_java_string_outputs = strings_to_java_string()
@trigger_message_outputs = trigger_message()
@htrc_page_retriever_outputs = htrc_page_retriever()
@text_accumulator_outputs = text_accumulator()
@simpletraintest_outputs = simpletraintest()
@csv_to_tuple_outputs = csv_to_tuple()
@tuple_value_to_string_2_outputs = tuple_value_to_string_2()
@stream_filter_3_outputs = stream_filter_3()
@stream_filter_2_outputs = stream_filter_2()
@flow_parameter_outputs = flow_parameter()
@stream_filter_outputs = stream_filter()
@push_text_2_outputs = push_text_2()
@trigger_stream_passthrough_outputs = trigger_stream_passthrough()
@push_text_3_outputs = push_text_3()
@table_to_csv_3_outputs = table_to_csv_3()
@push_text_4_outputs = push_text_4()
@table_to_csv_2_outputs = table_to_csv_2()
@table_to_csv_4_outputs = table_to_csv_4()
@prediction_report_table_2_outputs = prediction_report_table_2()
@createbintree_outputs = createbintree()
@text_cleaner_outputs = text_cleaner()
@tuple_value_frequency_counter_outputs = tuple_value_frequency_counter() [+AUTO!]
@createnbmodel_outputs = createnbmodel()
@table_to_csv_outputs = table_to_csv()
@text_accumulator_2_outputs = text_accumulator_2()
@search_text_2_outputs = search_text_2()

autobin(exampleTable: fork_x5_outputs.object)
write_to_file_4(
	location: push_text_outputs.text;
	data: table_to_csv_4_outputs.text
)
write_to_file_3(
	location: push_text_2_outputs.text;
	data: table_to_csv_3_outputs.text
)
write_to_file_2(
	location: push_text_3_outputs.text;
	data: table_to_csv_outputs.text
)
model_predict_2(
	model: fork_x2_3_outputs.object;
	table: simpletraintest_outputs.testTable
)
trigger_message_2(
	object: universal_text_extractor_2_outputs.text;
	trigger: stream_filter_3_outputs.object
)
csv_to_token_counts(text: tuple_to_csv_outputs.text)
featurefilterlite(sparseTable: stream_filter_outputs.object)
read_text(location: flow_parameter_outputs.text)
text_replacement(
	mapData: trigger_message_2_outputs.object;
	text: trigger_message_2_outputs.trigger
)
pos_tagger(tokenized_sentences: sentence_tokenizer_outputs.tokenized_sentences)
tuple_value_to_string(
	meta_tuple: csv_to_tuple_2_outputs.meta_tuple;
	tuples: csv_to_tuple_2_outputs.tuples
)
confusion_matrix(table: fork_x2_5_outputs.object)
tuple_to_csv(
	tuples: tuple_value_frequency_counter_outputs.tuples;
	meta_tuple: tuple_value_frequency_counter_outputs.meta_tuple
)
sentence_tokenizer(sentences: sentence_detector_outputs.sentences)
confusion_matrix_2(table: fork_x2_outputs.object)
csv_to_tuple_3(text: trigger_stream_passthrough_outputs.object)
csv_to_tuple_2(text: fork_x2_4_outputs.object2)
write_to_file(
	location: push_text_4_outputs.text;
	data: table_to_csv_2_outputs.text
)
text_cleaner_2(text: random_sample_outputs.text)
token_count_to_feature_table(
	id: fork_x2_2_outputs.object2;
	token_counts: csv_to_token_counts_outputs.token_counts;
	label: tuple_value_to_string_2_outputs.text
)
fork_x2_5(object: trigger_message_outputs.object)
fork_x2_4(object: read_text_outputs.text)
fork_x2_3(object: createnbmodel_outputs.nbModel)
search_text(text: text_cleaner_2_outputs.text)
fork_x2_2(object: stream_filter_2_outputs.object)
fork_x5(object: simpletraintest_outputs.trainTable)
random_sample(text: strings_to_java_string_outputs.java_string)
fork_x2(object: trigger_message_outputs.trigger)
model_predict(
	model: fork_x2_3_outputs.object2;
	table: fork_x5_outputs.object4
)
tuple_labeler(
	meta_tuple: csv_to_tuple_outputs.meta_tuple;
	tuples: csv_to_tuple_outputs.tuples;
	hashMapTuples: csv_to_tuple_3_outputs.tuples;
	hashMapMetaTuple: csv_to_tuple_3_outputs.meta_tuple
)
universal_text_extractor_2(location: flow_parameter_2_outputs.text)
prediction_report_table(table: fork_x2_outputs.object2)
sentence_detector(text: text_replacement_outputs.text)
strings_to_java_string(text: tuple_value_to_string_outputs.text)
trigger_message(
	object: model_predict_2_outputs.table;
	trigger: model_predict_outputs.table
)
htrc_page_retriever(volume_id_list: search_text_outputs.text_found)
text_accumulator(text: text_cleaner_outputs.text)
tuple_value_to_string_2(
	tuples: tuple_labeler_outputs.tuples;
	meta_tuple: tuple_labeler_outputs.meta_tuple
)
simpletraintest(originalTable: featurefilterlite_outputs.sparseTable)
csv_to_tuple(text: trigger_stream_passthrough_outputs.trigger)
stream_filter_3(object: text_accumulator_outputs.text)
stream_filter_2(object: search_text_2_outputs.text_found)
stream_filter(object: token_count_to_feature_table_outputs.table)
trigger_stream_passthrough(
	trigger: fork_x2_2_outputs.object;
	object: fork_x2_4_outputs.object
)
table_to_csv_3(table: prediction_report_table_2_outputs.table)
table_to_csv_2(table: prediction_report_table_outputs.table)
table_to_csv_4(table: confusion_matrix_outputs.table)
prediction_report_table_2(table: fork_x2_5_outputs.object2)
createbintree(
	binTransform: autobin_outputs.binTransform;
	exampleTable: fork_x5_outputs.object2
)
text_cleaner(text: htrc_page_retriever_outputs.text)
tuple_value_frequency_counter(
	tuples: pos_tagger_outputs.tuples;
	meta_tuple: pos_tagger_outputs.meta_tuple
)
createnbmodel(
	exampleTable: fork_x5_outputs.object3;
	binTree: createbintree_outputs.binTree
)
table_to_csv(table: confusion_matrix_2_outputs.table)
text_accumulator_2(text: htrc_page_retriever_outputs.volume_id)
search_text_2(text: text_accumulator_2_outputs.text)

