#
# Generated by RDF2ZZConverter v1.4.9.8618 on Tue Aug 27 19:25:19 CDT 2013
#
# @name 	HTRC_Dunning_LogLikelihood_to_Tagcloud
# @description 	<br>
# @creator 	admin
# @date 	Tue Aug 27 19:12:41 CDT 2013
# @rights 	UIUC/NCSA Open Source License
# @tags 	tag cloud, dunning, htrc
# @uri  	meandre://htrc.illinois.edu/flows/htrc_dunning_loglikelihood_to_tagcloud/
#

#
# Specify component imports
#
# TODO: Add component import statement(s) here
# Example: import <URL>   (replace 'URL' with the correct location)
import <http://repository.seasr.org/Meandre/Locations/Latest/Components/repository_components.nt>
import <http://repository.seasr.org/Meandre/Locations/Latest/HTRC_Components/repository_components.nt>

#
# Create the component aliases
#
alias <meandre://seasr.org/components/foundry/tuple-value-to-string> as TUPLE_VALUE_TO_STRING
alias <meandre://seasr.org/components/foundry/dunning-log-likelihood> as DUNNING_LOG_LIKELIHOOD
alias <meandre://seasr.org/components/foundry/write-to-file> as WRITE_TO_FILE
alias <meandre://seasr.org/components/foundry/random-sample> as RANDOM_SAMPLE
alias <meandre://seasr.org/components/foundry/token-counter-reducer> as TOKEN_COUNTER_REDUCER
alias <meandre://seasr.org/components/foundry/search-text> as SEARCH_TEXT
alias <meandre://seasr.org/components/foundry/strings-to-java-string> as STRINGS_TO_JAVA_STRING
alias <meandre://seasr.org/components/foundry/fork-x2> as FORK_X2
alias <meandre://seasr.org/components/foundry/tag-cloud> as TAG_CLOUD
alias <meandre://seasr.org/components/foundry/text-cleaner> as TEXT_CLEANER
alias <meandre://seasr.org/components/htrc/htrc-page-retriever> as HTRC_PAGE_RETRIEVER
alias <meandre://seasr.org/components/foundry/read-text> as READ_TEXT
alias <meandre://seasr.org/components/foundry/opennlp-sentence-detector> as OPENNLP_SENTENCE_DETECTOR
alias <meandre://seasr.org/components/foundry/opennlp-pos-tagger> as OPENNLP_POS_TAGGER
alias <meandre://seasr.org/components/foundry/push-text> as PUSH_TEXT
alias <meandre://seasr.org/components/foundry/token-counts-to-csv> as TOKEN_COUNTS_TO_CSV
alias <meandre://seasr.org/components/foundry/flow-parameter> as FLOW_PARAMETER
alias <meandre://seasr.org/components/foundry/csv-to-tuple> as CSV_TO_TUPLE
alias <meandre://seasr.org/components/foundry/tuple-value-frequency-counter> as TUPLE_VALUE_FREQUENCY_COUNTER
alias <meandre://seasr.org/components/foundry/text-accumulator> as TEXT_ACCUMULATOR
alias <meandre://seasr.org/components/foundry/tuple-to-csv> as TUPLE_TO_CSV
alias <meandre://seasr.org/components/foundry/csv-to-token-counts> as CSV_TO_TOKEN_COUNTS
alias <meandre://seasr.org/components/foundry/opennlp-sentence-tokenizer> as OPENNLP_SENTENCE_TOKENIZER
alias <meandre://seasr.org/components/foundry/top-n-filter> as TOP_N_FILTER
alias <meandre://seasr.org/components/foundry/stream-delimiter-filter> as STREAM_DELIMITER_FILTER

#
# Create the component instances
#
push_text_2 = PUSH_TEXT()
tag_cloud_2 = TAG_CLOUD()
token_counter_reducer_2 = TOKEN_COUNTER_REDUCER()
random_sample = RANDOM_SAMPLE()
stream_delimiter_filter = STREAM_DELIMITER_FILTER()
opennlp_sentence_tokenizer = OPENNLP_SENTENCE_TOKENIZER()
csv_to_token_counts_2 = CSV_TO_TOKEN_COUNTS()
text_accumulator = TEXT_ACCUMULATOR()
flow_parameter_2 = FLOW_PARAMETER()
write_to_file_2 = WRITE_TO_FILE()
text_cleaner_3 = TEXT_CLEANER()
flow_parameter = FLOW_PARAMETER()
push_text_3 = PUSH_TEXT()
tuple_value_frequency_counter = TUPLE_VALUE_FREQUENCY_COUNTER()
write_to_file_3 = WRITE_TO_FILE()
text_cleaner = TEXT_CLEANER()
top_n_filter = TOP_N_FILTER()
opennlp_sentence_detector = OPENNLP_SENTENCE_DETECTOR()
opennlp_pos_tagger = OPENNLP_POS_TAGGER()
token_counter_reducer = TOKEN_COUNTER_REDUCER()
htrc_page_retriever_2 = HTRC_PAGE_RETRIEVER()
write_to_file = WRITE_TO_FILE()
tuple_value_to_string = TUPLE_VALUE_TO_STRING()
token_counts_to_csv = TOKEN_COUNTS_TO_CSV()
tuple_value_to_string_2 = TUPLE_VALUE_TO_STRING()
tag_cloud = TAG_CLOUD()
fork_x2_3 = FORK_X2()
dunning_log_likelihood = DUNNING_LOG_LIKELIHOOD()
write_to_file_4 = WRITE_TO_FILE()
fork_x2_2 = FORK_X2()
stream_delimiter_filter_2 = STREAM_DELIMITER_FILTER()
top_n_filter_2 = TOP_N_FILTER()
token_counts_to_csv_2 = TOKEN_COUNTS_TO_CSV()
csv_to_tuple = CSV_TO_TUPLE()
stream_delimiter_filter_3 = STREAM_DELIMITER_FILTER()
read_text = READ_TEXT()
opennlp_sentence_detector_2 = OPENNLP_SENTENCE_DETECTOR()
fork_x2 = FORK_X2()
strings_to_java_string = STRINGS_TO_JAVA_STRING()
read_text_2 = READ_TEXT()
opennlp_sentence_tokenizer_2 = OPENNLP_SENTENCE_TOKENIZER()
csv_to_tuple_2 = CSV_TO_TUPLE()
text_cleaner_2 = TEXT_CLEANER()
tuple_to_csv_2 = TUPLE_TO_CSV()
htrc_page_retriever = HTRC_PAGE_RETRIEVER()
search_text = SEARCH_TEXT()
push_text_4 = PUSH_TEXT()
push_text = PUSH_TEXT()
text_cleaner_4 = TEXT_CLEANER()
csv_to_token_counts = CSV_TO_TOKEN_COUNTS()
tuple_value_frequency_counter_2 = TUPLE_VALUE_FREQUENCY_COUNTER()
text_accumulator_2 = TEXT_ACCUMULATOR()
opennlp_pos_tagger_2 = OPENNLP_POS_TAGGER()
strings_to_java_string_2 = STRINGS_TO_JAVA_STRING()
tuple_to_csv = TUPLE_TO_CSV()
random_sample_2 = RANDOM_SAMPLE()
search_text_2 = SEARCH_TEXT()

#
# Set component properties
#
push_text_2._debug_level = "info"
push_text_2.message = "dunning_over_represented.csv"
push_text_2._ignore_errors = "false"

tag_cloud_2.show_tooltip = "true"
tag_cloud_2.d3_api_url = "http://d3js.org/d3.v2.min.js"
tag_cloud_2.show_counts = "false"
tag_cloud_2.max_size = "150"
tag_cloud_2.force_positive = "true"
tag_cloud_2._ignore_errors = "false"
tag_cloud_2.d3_cloud_api_url = "http://www.jasondavies.com/wordcloud/d3.layout.cloud.js"
tag_cloud_2.properties = "rotation=0"
tag_cloud_2.css = ""
tag_cloud_2._debug_level = "info"
tag_cloud_2.height = "1000"
tag_cloud_2.width = "1000"
tag_cloud_2.font_name = ""
tag_cloud_2.color_palette = "category20"
tag_cloud_2.min_size = "20"
tag_cloud_2.template = "org/seasr/meandre/components/vis/d3/TagCloud.vm"
tag_cloud_2.scale = "log"
tag_cloud_2.title = "Tagcloud"

token_counter_reducer_2.ordered = "true"
token_counter_reducer_2._debug_level = "info"
token_counter_reducer_2._stream_id = "3"
token_counter_reducer_2._ignore_errors = "false"

random_sample.seed = "123"
random_sample._debug_level = "info"
random_sample.count = "10"
random_sample._ignore_errors = "false"

stream_delimiter_filter.advanced_filter = ""
stream_delimiter_filter._debug_level = "info"
stream_delimiter_filter._stream_id = "2"
stream_delimiter_filter._ignore_errors = "false"

opennlp_sentence_tokenizer._debug_level = "info"
opennlp_sentence_tokenizer.lang_code = "en"
opennlp_sentence_tokenizer._ignore_errors = "false"

csv_to_token_counts_2.enable_constraint_checking = "false"
csv_to_token_counts_2.column_count = "2"
csv_to_token_counts_2._ignore_errors = "false"
csv_to_token_counts_2.header = "true"
csv_to_token_counts_2.ordered = "true"
csv_to_token_counts_2.token_pos = "0"
csv_to_token_counts_2.count_pos = "1"
csv_to_token_counts_2._debug_level = "info"

text_accumulator._debug_level = "info"
text_accumulator.separator = " "
text_accumulator._stream_id = "2"
text_accumulator._ignore_errors = "false"

flow_parameter_2.default_value = "/home/lauvil/HTRC/data/volume_id_dunning_reference.txt"
flow_parameter_2._debug_level = "info"
flow_parameter_2.param_name = "volume_id_dunning_reference"
flow_parameter_2._stream_id = "99"
flow_parameter_2._ignore_errors = "false"
flow_parameter_2.wrap_stream = "false"

write_to_file_2._debug_level = "info"
write_to_file_2.append_timestamp = "false"
write_to_file_2.append_data_to_file = "false"
write_to_file_2._ignore_errors = "false"
write_to_file_2.default_folder = "/tmp"

text_cleaner_3.replace4 = ""
text_cleaner_3.replace3 = ""
text_cleaner_3.replace2 = ""
text_cleaner_3.replace = "|"
text_cleaner_3._ignore_errors = "false"
text_cleaner_3.find = "\\n"
text_cleaner_3.find4 = ""
text_cleaner_3.find3 = ""
text_cleaner_3.find2 = ""
text_cleaner_3._debug_level = "info"

flow_parameter.default_value = "/home/lauvil/HTRC/data/volume_id_dunning_analysis.txt"
flow_parameter._debug_level = "info"
flow_parameter.param_name = "volume_id_dunning_analysis"
flow_parameter._stream_id = "99"
flow_parameter.wrap_stream = "false"
flow_parameter._ignore_errors = "false"

push_text_3._debug_level = "info"
push_text_3.message = "dunning_tagcloud_under.html"
push_text_3._ignore_errors = "false"

tuple_value_frequency_counter.tupleField = "token"
tuple_value_frequency_counter._ignore_errors = "false"
tuple_value_frequency_counter.trim_fields = "token"
tuple_value_frequency_counter.threshold = "0"
tuple_value_frequency_counter.max_size = "-1"
tuple_value_frequency_counter.normalize_fields = "token"
tuple_value_frequency_counter._debug_level = "info"

write_to_file_3._debug_level = "info"
write_to_file_3.append_timestamp = "false"
write_to_file_3.append_data_to_file = "false"
write_to_file_3.default_folder = "/tmp"
write_to_file_3._ignore_errors = "false"

text_cleaner.replace4 = ""
text_cleaner.replace3 = ""
text_cleaner.replace2 = ""
text_cleaner.replace = ""
text_cleaner._ignore_errors = "false"
text_cleaner.find = "^(.*)\\s"
text_cleaner.find4 = ""
text_cleaner.find3 = "(?m)--?\\s*$\\s*"
text_cleaner.find2 = "(.*)$"
text_cleaner._debug_level = "info"

top_n_filter.ordered = "true"
top_n_filter._debug_level = "info"
top_n_filter.bottom_n = "false"
top_n_filter.n_top_tokens = "100"
top_n_filter._ignore_errors = "false"

opennlp_sentence_detector._debug_level = "info"
opennlp_sentence_detector.lang_code = "en"
opennlp_sentence_detector.remove_newline = "false"
opennlp_sentence_detector._ignore_errors = "false"

opennlp_pos_tagger._debug_level = "info"
opennlp_pos_tagger.lang_code = "en"
opennlp_pos_tagger.filter_regex = "NN|NNS|JJ.*|RB.*|PRP.*|RP|VB.*|IN"
opennlp_pos_tagger._ignore_errors = "false"

token_counter_reducer.ordered = "true"
token_counter_reducer._debug_level = "info"
token_counter_reducer._stream_id = "3"
token_counter_reducer._ignore_errors = "false"

htrc_page_retriever_2.delimiter = "|"
htrc_page_retriever_2.wrap_stream = "true"
htrc_page_retriever_2.read_timeout = "0"
htrc_page_retriever_2.stream_per_volume = "true"
htrc_page_retriever_2._ignore_errors = "false"
htrc_page_retriever_2.auth_selfsign = "false"
htrc_page_retriever_2.auth_token = "a6fa7057bfa0e5a9a33411e41ca9bc69"
htrc_page_retriever_2._stream_id = "2"
htrc_page_retriever_2.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever_2._debug_level = "info"
htrc_page_retriever_2.connection_timeout = "0"

write_to_file._debug_level = "info"
write_to_file.append_timestamp = "false"
write_to_file.append_data_to_file = "false"
write_to_file._ignore_errors = "false"
write_to_file.default_folder = "/tmp"

tuple_value_to_string._debug_level = "info"
tuple_value_to_string.attribute = "volume_id"
tuple_value_to_string._ignore_errors = "false"

token_counts_to_csv.ordered = "true"
token_counts_to_csv._debug_level = "info"
token_counts_to_csv.header = "tokens,counts"
token_counts_to_csv._ignore_errors = "false"

tuple_value_to_string_2._debug_level = "info"
tuple_value_to_string_2.attribute = "volume_id"
tuple_value_to_string_2._ignore_errors = "false"

tag_cloud.show_tooltip = "true"
tag_cloud.d3_api_url = "http://d3js.org/d3.v2.min.js"
tag_cloud.show_counts = "false"
tag_cloud.max_size = "150"
tag_cloud.force_positive = "true"
tag_cloud._ignore_errors = "false"
tag_cloud.d3_cloud_api_url = "http://www.jasondavies.com/wordcloud/d3.layout.cloud.js"
tag_cloud.properties = "rotation=0"
tag_cloud.css = ""
tag_cloud._debug_level = "info"
tag_cloud.height = "1000"
tag_cloud.width = "1000"
tag_cloud.font_name = ""
tag_cloud.color_palette = "category20"
tag_cloud.min_size = "20"
tag_cloud.template = "org/seasr/meandre/components/vis/d3/TagCloud.vm"
tag_cloud.scale = "log"
tag_cloud.title = "Tagcloud"

fork_x2_3.replication_mode = "2"
fork_x2_3._debug_level = "info"
fork_x2_3.replication_method_name = ""
fork_x2_3._ignore_errors = "false"

dunning_log_likelihood.signed_ints = "true"
dunning_log_likelihood.cutoff = "0"
dunning_log_likelihood._debug_level = "info"
dunning_log_likelihood.signed_doubles = "true"
dunning_log_likelihood._ignore_errors = "false"

write_to_file_4._debug_level = "info"
write_to_file_4.append_timestamp = "false"
write_to_file_4.append_data_to_file = "false"
write_to_file_4.default_folder = "/tmp"
write_to_file_4._ignore_errors = "false"

fork_x2_2.replication_mode = "0"
fork_x2_2.replication_method_name = ""
fork_x2_2._debug_level = "info"
fork_x2_2._ignore_errors = "false"

stream_delimiter_filter_2._debug_level = "info"
stream_delimiter_filter_2.advanced_filter = ""
stream_delimiter_filter_2._stream_id = ""
stream_delimiter_filter_2._ignore_errors = "false"

top_n_filter_2.bottom_n = "true"
top_n_filter_2.n_top_tokens = "100"
top_n_filter_2._ignore_errors = "false"
top_n_filter_2.ordered = "true"
top_n_filter_2._debug_level = "info"

token_counts_to_csv_2.ordered = "true"
token_counts_to_csv_2._debug_level = "info"
token_counts_to_csv_2._ignore_errors = "false"
token_counts_to_csv_2.header = "tokens,counts"

csv_to_tuple._debug_level = "info"
csv_to_tuple._ignore_errors = "false"
csv_to_tuple.labels = "volume_id"
csv_to_tuple.header = "false"

stream_delimiter_filter_3.advanced_filter = ""
stream_delimiter_filter_3._debug_level = "info"
stream_delimiter_filter_3._stream_id = "2"
stream_delimiter_filter_3._ignore_errors = "false"

read_text.retry_on_timeout = "true"
read_text.read_timeout = "0"
read_text.max_attempts = "1"
read_text._ignore_errors = "false"
read_text.retry_on_http_error = "0"
read_text.retry_delay = "1000"
read_text.connection_timeout = "0"
read_text._debug_level = "info"

opennlp_sentence_detector_2._debug_level = "info"
opennlp_sentence_detector_2.lang_code = "en"
opennlp_sentence_detector_2.remove_newline = "false"
opennlp_sentence_detector_2._ignore_errors = "false"

fork_x2.replication_mode = "0"
fork_x2._debug_level = "info"
fork_x2.replication_method_name = ""
fork_x2._ignore_errors = "false"

strings_to_java_string._debug_level = "info"
strings_to_java_string.separator = "\\n"
strings_to_java_string._ignore_errors = "false"

read_text_2.retry_on_timeout = "true"
read_text_2.read_timeout = "0"
read_text_2.max_attempts = "1"
read_text_2._ignore_errors = "false"
read_text_2.retry_on_http_error = "0"
read_text_2.retry_delay = "1000"
read_text_2.connection_timeout = "0"
read_text_2._debug_level = "info"

opennlp_sentence_tokenizer_2._debug_level = "info"
opennlp_sentence_tokenizer_2.lang_code = "en"
opennlp_sentence_tokenizer_2._ignore_errors = "false"

csv_to_tuple_2._debug_level = "info"
csv_to_tuple_2.header = "false"
csv_to_tuple_2.labels = "volume_id"
csv_to_tuple_2._ignore_errors = "false"

text_cleaner_2.replace4 = ""
text_cleaner_2.replace3 = ""
text_cleaner_2.replace2 = ""
text_cleaner_2.replace = ""
text_cleaner_2._ignore_errors = "false"
text_cleaner_2.find4 = ""
text_cleaner_2.find = "^(.*)\\s"
text_cleaner_2.find3 = "(?m)--?\\s*$\\s*"
text_cleaner_2.find2 = "(.*)$"
text_cleaner_2._debug_level = "info"

tuple_to_csv_2._debug_level = "info"
tuple_to_csv_2._ignore_errors = "false"
tuple_to_csv_2.header = "true"

htrc_page_retriever.delimiter = "|"
htrc_page_retriever.wrap_stream = "true"
htrc_page_retriever.read_timeout = "0"
htrc_page_retriever.stream_per_volume = "true"
htrc_page_retriever._ignore_errors = "false"
htrc_page_retriever.auth_selfsign = "false"
htrc_page_retriever.auth_token = "a6fa7057bfa0e5a9a33411e41ca9bc69"
htrc_page_retriever._stream_id = "2"
htrc_page_retriever.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever._debug_level = "info"
htrc_page_retriever.connection_timeout = "0"

search_text._debug_level = "info"
search_text._stream_id = "3"
search_text.expression = "(?:[^|]+\\|?){1,4}"
search_text._ignore_errors = "false"
search_text.wrap_stream = "true"

push_text_4._debug_level = "info"
push_text_4.message = "dunning_under_represented.csv"
push_text_4._ignore_errors = "false"

push_text._debug_level = "info"
push_text.message = "dunning_tagcloud_over.html"
push_text._ignore_errors = "false"

text_cleaner_4.replace4 = ""
text_cleaner_4.replace3 = ""
text_cleaner_4.replace2 = ""
text_cleaner_4.replace = "|"
text_cleaner_4._ignore_errors = "false"
text_cleaner_4.find = "\\n"
text_cleaner_4.find4 = ""
text_cleaner_4.find3 = ""
text_cleaner_4.find2 = ""
text_cleaner_4._debug_level = "info"

csv_to_token_counts.enable_constraint_checking = "false"
csv_to_token_counts.column_count = "2"
csv_to_token_counts._ignore_errors = "false"
csv_to_token_counts.header = "true"
csv_to_token_counts.ordered = "true"
csv_to_token_counts.token_pos = "0"
csv_to_token_counts.count_pos = "1"
csv_to_token_counts._debug_level = "info"

tuple_value_frequency_counter_2.tupleField = "token"
tuple_value_frequency_counter_2._ignore_errors = "false"
tuple_value_frequency_counter_2.trim_fields = "token"
tuple_value_frequency_counter_2.threshold = "0"
tuple_value_frequency_counter_2.max_size = "-1"
tuple_value_frequency_counter_2.normalize_fields = "token"
tuple_value_frequency_counter_2._debug_level = "info"

text_accumulator_2._debug_level = "info"
text_accumulator_2.separator = " "
text_accumulator_2._stream_id = "2"
text_accumulator_2._ignore_errors = "false"

opennlp_pos_tagger_2._debug_level = "info"
opennlp_pos_tagger_2.lang_code = "en"
opennlp_pos_tagger_2.filter_regex = "NN|NNS|JJ.*|RB.*|PRP.*|RP|VB.*|IN"
opennlp_pos_tagger_2._ignore_errors = "false"

strings_to_java_string_2._debug_level = "info"
strings_to_java_string_2.separator = "\\n"
strings_to_java_string_2._ignore_errors = "false"

tuple_to_csv._debug_level = "info"
tuple_to_csv._ignore_errors = "false"
tuple_to_csv.header = "true"

random_sample_2.seed = "123"
random_sample_2._debug_level = "info"
random_sample_2.count = "10"
random_sample_2._ignore_errors = "false"

search_text_2._debug_level = "info"
search_text_2._stream_id = "3"
search_text_2.expression = "(?:[^|]+\\|?){1,4}"
search_text_2.wrap_stream = "true"
search_text_2._ignore_errors = "false"

#
# Create the flow by connecting the components
#
@random_sample_outputs = random_sample()
@csv_to_token_counts_outputs = csv_to_token_counts()
@opennlp_sentence_detector_2_outputs = opennlp_sentence_detector_2() [+AUTO!]
@opennlp_pos_tagger_2_outputs = opennlp_pos_tagger_2() [+AUTO!]
@strings_to_java_string_outputs = strings_to_java_string()
@strings_to_java_string_2_outputs = strings_to_java_string_2()
@htrc_page_retriever_2_outputs = htrc_page_retriever_2()
@token_counts_to_csv_2_outputs = token_counts_to_csv_2()
@csv_to_tuple_2_outputs = csv_to_tuple_2()
@push_text_2_outputs = push_text_2()
@push_text_3_outputs = push_text_3()
@push_text_4_outputs = push_text_4()
@tuple_value_frequency_counter_2_outputs = tuple_value_frequency_counter_2() [+AUTO!]
@search_text_outputs = search_text()
@tuple_to_csv_outputs = tuple_to_csv()
@opennlp_sentence_detector_outputs = opennlp_sentence_detector() [+AUTO!]
@csv_to_tuple_outputs = csv_to_tuple()
@flow_parameter_outputs = flow_parameter()
@tuple_to_csv_2_outputs = tuple_to_csv_2()
@stream_delimiter_filter_outputs = stream_delimiter_filter()
@text_accumulator_2_outputs = text_accumulator_2()
@fork_x2_outputs = fork_x2()
@read_text_outputs = read_text()
@stream_delimiter_filter_3_outputs = stream_delimiter_filter_3()
@text_cleaner_4_outputs = text_cleaner_4()
@text_cleaner_3_outputs = text_cleaner_3()
@text_cleaner_outputs = text_cleaner() [+AUTO!]
@stream_delimiter_filter_2_outputs = stream_delimiter_filter_2()
@text_cleaner_2_outputs = text_cleaner_2() [+AUTO!]
@dunning_log_likelihood_outputs = dunning_log_likelihood()
@search_text_2_outputs = search_text_2()
@tuple_value_to_string_2_outputs = tuple_value_to_string_2()
@token_counter_reducer_outputs = token_counter_reducer()
@tuple_value_frequency_counter_outputs = tuple_value_frequency_counter() [+AUTO!]
@token_counts_to_csv_outputs = token_counts_to_csv()
@top_n_filter_2_outputs = top_n_filter_2()
@top_n_filter_outputs = top_n_filter()
@tag_cloud_outputs = tag_cloud()
@fork_x2_3_outputs = fork_x2_3()
@fork_x2_2_outputs = fork_x2_2()
@text_accumulator_outputs = text_accumulator()
@push_text_outputs = push_text()
@tuple_value_to_string_outputs = tuple_value_to_string()
@read_text_2_outputs = read_text_2()
@opennlp_sentence_tokenizer_outputs = opennlp_sentence_tokenizer() [+AUTO!]
@opennlp_pos_tagger_outputs = opennlp_pos_tagger() [+AUTO!]
@tag_cloud_2_outputs = tag_cloud_2()
@flow_parameter_2_outputs = flow_parameter_2()
@random_sample_2_outputs = random_sample_2()
@token_counter_reducer_2_outputs = token_counter_reducer_2()
@htrc_page_retriever_outputs = htrc_page_retriever()
@opennlp_sentence_tokenizer_2_outputs = opennlp_sentence_tokenizer_2() [+AUTO!]
@csv_to_token_counts_2_outputs = csv_to_token_counts_2()

random_sample(text: strings_to_java_string_outputs.java_string)
csv_to_token_counts(text: tuple_to_csv_outputs.text)
opennlp_sentence_detector_2(text: stream_delimiter_filter_outputs.object)
opennlp_pos_tagger_2(tokenized_sentences: opennlp_sentence_tokenizer_2_outputs.tokenized_sentences)
strings_to_java_string(text: tuple_value_to_string_2_outputs.text)
strings_to_java_string_2(text: tuple_value_to_string_outputs.text)
htrc_page_retriever_2(volume_id_list: search_text_2_outputs.text_found)
token_counts_to_csv_2(token_counts: fork_x2_2_outputs.object2)
csv_to_tuple_2(text: read_text_2_outputs.text)
tuple_value_frequency_counter_2(
	tuples: opennlp_pos_tagger_2_outputs.tuples;
	meta_tuple: opennlp_pos_tagger_2_outputs.meta_tuple
)
search_text(text: text_cleaner_3_outputs.text)
tuple_to_csv(
	tuples: tuple_value_frequency_counter_outputs.tuples;
	meta_tuple: tuple_value_frequency_counter_outputs.meta_tuple
)
opennlp_sentence_detector(text: stream_delimiter_filter_3_outputs.object)
csv_to_tuple(text: read_text_outputs.text)
stream_delimiter_filter(object: text_accumulator_2_outputs.text)
tuple_to_csv_2(
	tuples: tuple_value_frequency_counter_2_outputs.tuples;
	meta_tuple: tuple_value_frequency_counter_2_outputs.meta_tuple
)
text_accumulator_2(text: text_cleaner_2_outputs.text)
read_text(location: flow_parameter_outputs.text)
fork_x2(object: top_n_filter_outputs.token_counts)
stream_delimiter_filter_3(object: text_accumulator_outputs.text)
text_cleaner_4(text: random_sample_2_outputs.text)
text_cleaner_3(text: random_sample_outputs.text)
text_cleaner(text: htrc_page_retriever_outputs.text)
stream_delimiter_filter_2(object: dunning_log_likelihood_outputs.int_likelihood_scores)
text_cleaner_2(text: htrc_page_retriever_2_outputs.text)
dunning_log_likelihood(
	token_counts_reference: token_counter_reducer_2_outputs.token_counts;
	token_counts: token_counter_reducer_outputs.token_counts
)
search_text_2(text: text_cleaner_4_outputs.text)
token_counter_reducer(token_counts: csv_to_token_counts_outputs.token_counts)
tuple_value_to_string_2(
	meta_tuple: csv_to_tuple_outputs.meta_tuple;
	tuples: csv_to_tuple_outputs.tuples
)
tuple_value_frequency_counter(
	meta_tuple: opennlp_pos_tagger_outputs.meta_tuple;
	tuples: opennlp_pos_tagger_outputs.tuples
)
token_counts_to_csv(token_counts: fork_x2_outputs.object2)
top_n_filter_2(token_counts: fork_x2_3_outputs.object2)
top_n_filter(token_counts: fork_x2_3_outputs.object)
tag_cloud(token_counts: fork_x2_outputs.object)
fork_x2_3(object: stream_delimiter_filter_2_outputs.object)
fork_x2_2(object: top_n_filter_2_outputs.token_counts)
text_accumulator(text: text_cleaner_outputs.text)
tuple_value_to_string(
	meta_tuple: csv_to_tuple_2_outputs.meta_tuple;
	tuples: csv_to_tuple_2_outputs.tuples
)
read_text_2(location: flow_parameter_2_outputs.text)
opennlp_sentence_tokenizer(sentences: opennlp_sentence_detector_outputs.sentences)
opennlp_pos_tagger(tokenized_sentences: opennlp_sentence_tokenizer_outputs.tokenized_sentences)
tag_cloud_2(token_counts: fork_x2_2_outputs.object)
random_sample_2(text: strings_to_java_string_2_outputs.java_string)
write_to_file(
	data: tag_cloud_outputs.html;
	location: push_text_outputs.text
)
token_counter_reducer_2(token_counts: csv_to_token_counts_2_outputs.token_counts)
write_to_file_2(
	data: token_counts_to_csv_outputs.text;
	location: push_text_2_outputs.text
)
htrc_page_retriever(volume_id_list: search_text_outputs.text_found)
write_to_file_4(
	location: push_text_4_outputs.text;
	data: token_counts_to_csv_2_outputs.text
)
write_to_file_3(
	location: push_text_3_outputs.text;
	data: tag_cloud_2_outputs.html
)
opennlp_sentence_tokenizer_2(sentences: opennlp_sentence_detector_2_outputs.sentences)
csv_to_token_counts_2(text: tuple_to_csv_2_outputs.text)

