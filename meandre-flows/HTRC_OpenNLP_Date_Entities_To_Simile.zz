#
# Generated by RDF2ZZConverter v1.4.11.8693M on Mon Mar 23 14:20:30 CDT 2015
#
# @name 	HTRC_OpenNLP_Date_Entities_To_Simile
# @description 	<br>
# @creator 	admin
# @date 	Tue Aug 27 19:28:44 CDT 2013
# @rights 	UIUC/NCSA Open Source License
# @tags 	nlp, simile, entities, timeline, htrc, entity
# @uri  	meandre://htrc.illinois.edu/flows/htrc_opennlp_date_entities_to_simile/
#

#
# Specify component imports
#
# TODO: Add component import statement(s) here
# Example: import <URL>   (replace 'URL' with the correct location)
import <http://dev5.informatics.illinois.edu:1714/public/services/repository.ttl>
#
# Create the component aliases
#
alias <meandre://seasr.org/components/foundry/date-filter> as DATE_FILTER
alias <meandre://seasr.org/components/foundry/fork-x2> as FORK_X2
alias <meandre://seasr.org/components/foundry/opennlp-sentence-detector> as OPENNLP_SENTENCE_DETECTOR
alias <meandre://seasr.org/components/foundry/csv-to-tuple> as CSV_TO_TUPLE
alias <meandre://seasr.org/components/foundry/tuple-value-to-string> as TUPLE_VALUE_TO_STRING
alias <meandre://seasr.org/components/foundry/concatenate-text> as CONCATENATE_TEXT
alias <meandre://seasr.org/components/foundry/opennlp-named-entity> as OPENNLP_NAMED_ENTITY
alias <meandre://seasr.org/components/foundry/entity-xml-to-simile-xml> as ENTITY_XML_TO_SIMILE_XML
alias <meandre://seasr.org/components/foundry/simile-timeline-generator> as SIMILE_TIMELINE_GENERATOR
alias <meandre://seasr.org/components/foundry/search-text> as SEARCH_TEXT
alias <meandre://seasr.org/components/foundry/text-cleaner> as TEXT_CLEANER
alias <meandre://seasr.org/components/foundry/write-to-file> as WRITE_TO_FILE
alias <meandre://seasr.org/components/foundry/strings-to-java-string> as STRINGS_TO_JAVA_STRING
alias <meandre://seasr.org/components/foundry/xml-to-xml-with-xsl-2> as XML_TO_XML_WITH_XSL_2
alias <meandre://seasr.org/components/foundry/tuple-to-entity-xml> as TUPLE_TO_ENTITY_XML
alias <meandre://seasr.org/components/foundry/push-text> as PUSH_TEXT
alias <meandre://seasr.org/components/foundry/flow-parameter> as FLOW_PARAMETER
alias <meandre://seasr.org/components/foundry/random-sample> as RANDOM_SAMPLE
alias <meandre://seasr.org/components/htrc/htrc-page-retriever> as HTRC_PAGE_RETRIEVER
alias <meandre://seasr.org/components/foundry/read-text> as READ_TEXT
alias <meandre://seasr.org/components/foundry/opennlp-sentence-tokenizer> as OPENNLP_SENTENCE_TOKENIZER
alias <meandre://seasr.org/components/foundry/stream-delimiter-filter> as STREAM_DELIMITER_FILTER

#
# Create the component instances
#
tuple_value_to_string = TUPLE_VALUE_TO_STRING()
date_filter = DATE_FILTER()
concatenate_text = CONCATENATE_TEXT()
entity_xml_to_simile_xml = ENTITY_XML_TO_SIMILE_XML()
text_cleaner_2 = TEXT_CLEANER()
write_to_file = WRITE_TO_FILE()
csv_to_tuple = CSV_TO_TUPLE()
flow_parameter = FLOW_PARAMETER()
text_cleaner_3 = TEXT_CLEANER()
stream_delimiter_filter = STREAM_DELIMITER_FILTER()
simile_timeline_generator = SIMILE_TIMELINE_GENERATOR()
text_cleaner = TEXT_CLEANER()
fork_x2 = FORK_X2()
xml_to_xml_with_xsl_2 = XML_TO_XML_WITH_XSL_2()
opennlp_sentence_detector = OPENNLP_SENTENCE_DETECTOR()
strings_to_java_string = STRINGS_TO_JAVA_STRING()
htrc_page_retriever = HTRC_PAGE_RETRIEVER()
text_cleaner_4 = TEXT_CLEANER()
push_text = PUSH_TEXT()
opennlp_sentence_tokenizer = OPENNLP_SENTENCE_TOKENIZER()
tuple_to_entity_xml = TUPLE_TO_ENTITY_XML()
read_text = READ_TEXT()
opennlp_named_entity = OPENNLP_NAMED_ENTITY()
search_text = SEARCH_TEXT()
random_sample = RANDOM_SAMPLE()

#
# Set component properties
#
tuple_value_to_string._debug_level = "info"
tuple_value_to_string.attribute = "volume_id"
tuple_value_to_string._ignore_errors = "false"

date_filter.min_value = "1800"
date_filter._debug_level = "info"
date_filter.max_value = "1900"
date_filter._ignore_errors = "false"

concatenate_text._debug_level = "info"
concatenate_text.separator = ":"
concatenate_text._ignore_errors = "false"

entity_xml_to_simile_xml._debug_level = "info"
entity_xml_to_simile_xml._ignore_errors = "false"

text_cleaner_2.replace4 = " ) "
text_cleaner_2.replace3 = " ( "
text_cleaner_2.replace2 = " ] "
text_cleaner_2.replace = " [ "
text_cleaner_2._ignore_errors = "false"
text_cleaner_2.find = "\\["
text_cleaner_2.find4 = "\\)"
text_cleaner_2.find3 = "\\("
text_cleaner_2.find2 = "\\]"
text_cleaner_2._debug_level = "info"

write_to_file._debug_level = "info"
write_to_file.append_timestamp = "false"
write_to_file.append_data_to_file = "false"
write_to_file._ignore_errors = "false"
write_to_file.default_folder = "/tmp"

csv_to_tuple._debug_level = "info"
csv_to_tuple.labels = "volume_id"
csv_to_tuple._ignore_errors = "false"
csv_to_tuple.header = "false"

flow_parameter.default_value = "/home/lauvil/HTRC/data/volume_id.txt"
flow_parameter._debug_level = "info"
flow_parameter.param_name = "volume_id"
flow_parameter._stream_id = "99"
flow_parameter.wrap_stream = "false"
flow_parameter._ignore_errors = "false"

text_cleaner_3.replace4 = ""
text_cleaner_3.replace3 = ""
text_cleaner_3.replace2 = ""
text_cleaner_3.replace = " , "
text_cleaner_3._ignore_errors = "false"
text_cleaner_3.find = ","
text_cleaner_3.find4 = ""
text_cleaner_3.find3 = ""
text_cleaner_3.find2 = ""
text_cleaner_3._debug_level = "info"

stream_delimiter_filter.advanced_filter = ""
stream_delimiter_filter._debug_level = "info"
stream_delimiter_filter._stream_id = ""
stream_delimiter_filter._ignore_errors = "false"

simile_timeline_generator.save_output_to_file = "false"
simile_timeline_generator._ignore_errors = "false"
simile_timeline_generator.simile_ref_url = ""
simile_timeline_generator.output_folder = "../simile"
simile_timeline_generator.timeline_api_url = "http://www.simile-widgets.org/timeline/api/timeline-api.js"
simile_timeline_generator.timeline_css = "height: 550px; border: 1px solid #aaa"
simile_timeline_generator.inline_simile_xml = "true"
simile_timeline_generator._debug_level = "info"

text_cleaner.replace4 = " . "
text_cleaner.replace3 = ""
text_cleaner.replace2 = ""
text_cleaner.replace = ""
text_cleaner._ignore_errors = "false"
text_cleaner.find = "^(.*)\\s"
text_cleaner.find4 = "\\."
text_cleaner.find3 = "(?m)--?\\s*$\\s*"
text_cleaner.find2 = "(.*)$"
text_cleaner._debug_level = "info"

fork_x2.replication_mode = "0"
fork_x2._debug_level = "info"
fork_x2.replication_method_name = ""
fork_x2._ignore_errors = "false"

xml_to_xml_with_xsl_2._debug_level = "info"
xml_to_xml_with_xsl_2._ignore_errors = "false"

opennlp_sentence_detector._debug_level = "info"
opennlp_sentence_detector.lang_code = "en"
opennlp_sentence_detector.remove_newline = "false"
opennlp_sentence_detector._ignore_errors = "false"

strings_to_java_string._debug_level = "info"
strings_to_java_string.separator = "\\n"
strings_to_java_string._ignore_errors = "false"

htrc_page_retriever.delimiter = "|"
htrc_page_retriever.wrap_stream = "false"
htrc_page_retriever.read_timeout = "0"
htrc_page_retriever.stream_per_volume = "false"
htrc_page_retriever._ignore_errors = "false"
htrc_page_retriever.auth_selfsign = "false"
htrc_page_retriever.auth_token = "a6fa7057bfa0e5a9a33411e41ca9bc69"
htrc_page_retriever._stream_id = "99"
htrc_page_retriever.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever._debug_level = "info"
htrc_page_retriever.connection_timeout = "0"

text_cleaner_4.replace4 = ""
text_cleaner_4.replace3 = ""
text_cleaner_4.replace2 = ""
text_cleaner_4.replace = "|"
text_cleaner_4._ignore_errors = "false"
text_cleaner_4.find4 = ""
text_cleaner_4.find = "\\n"
text_cleaner_4.find3 = ""
text_cleaner_4.find2 = ""
text_cleaner_4._debug_level = "info"

push_text._debug_level = "info"
push_text.message = "date_entity_simile.html"
push_text._ignore_errors = "false"

opennlp_sentence_tokenizer._debug_level = "info"
opennlp_sentence_tokenizer.lang_code = "en"
opennlp_sentence_tokenizer._ignore_errors = "false"

tuple_to_entity_xml._debug_level = "info"
tuple_to_entity_xml.number = "3"
tuple_to_entity_xml.encoding = "UTF-8"
tuple_to_entity_xml.entities = "date"
tuple_to_entity_xml._stream_id = "1"
tuple_to_entity_xml._ignore_errors = "false"

read_text.retry_on_timeout = "true"
read_text.read_timeout = "0"
read_text.max_attempts = "1"
read_text._ignore_errors = "false"
read_text.retry_on_http_error = "0"
read_text.retry_delay = "1000"
read_text.connection_timeout = "0"
read_text._debug_level = "info"

opennlp_named_entity.entity_types = "date"
opennlp_named_entity._debug_level = "info"
opennlp_named_entity.lang_code = "en"
opennlp_named_entity._ignore_errors = "false"

search_text._debug_level = "info"
search_text._stream_id = "1"
search_text.expression = "(?:[^|]+\\|?){1,4}"
search_text.wrap_stream = "true"
search_text._ignore_errors = "false"

random_sample.seed = "123"
random_sample._debug_level = "info"
random_sample.count = "10"
random_sample._ignore_errors = "false"

#
# Create the flow by connecting the components
#
@flow_parameter_outputs = flow_parameter()
@opennlp_sentence_detector_outputs = opennlp_sentence_detector() [+AUTO!]
@entity_xml_to_simile_xml_outputs = entity_xml_to_simile_xml()
@random_sample_outputs = random_sample()
@xml_to_xml_with_xsl_2_outputs = xml_to_xml_with_xsl_2()
@fork_x2_outputs = fork_x2()
@opennlp_named_entity_outputs = opennlp_named_entity() [+AUTO!]
@text_cleaner_outputs = text_cleaner()
@text_cleaner_2_outputs = text_cleaner_2()
@stream_delimiter_filter_outputs = stream_delimiter_filter()
@date_filter_outputs = date_filter()
@concatenate_text_outputs = concatenate_text()
@strings_to_java_string_outputs = strings_to_java_string()
@read_text_outputs = read_text()
@tuple_to_entity_xml_outputs = tuple_to_entity_xml()
@opennlp_sentence_tokenizer_outputs = opennlp_sentence_tokenizer() [+AUTO!]
@htrc_page_retriever_outputs = htrc_page_retriever()
@tuple_value_to_string_outputs = tuple_value_to_string()
@csv_to_tuple_outputs = csv_to_tuple()
@push_text_outputs = push_text()
@text_cleaner_4_outputs = text_cleaner_4()
@simile_timeline_generator_outputs = simile_timeline_generator()
@text_cleaner_3_outputs = text_cleaner_3()
@search_text_outputs = search_text()

opennlp_sentence_detector(text: text_cleaner_3_outputs.text)
entity_xml_to_simile_xml(xml: stream_delimiter_filter_outputs.object)
random_sample(text: strings_to_java_string_outputs.java_string)
xml_to_xml_with_xsl_2(
	xsl: date_filter_outputs.xsl;
	xml: entity_xml_to_simile_xml_outputs.xml
)
fork_x2(object: opennlp_sentence_tokenizer_outputs.tokenized_sentences)
text_cleaner(text: htrc_page_retriever_outputs.text)
opennlp_named_entity(tokenized_sentences: fork_x2_outputs.object)
write_to_file(
	location: push_text_outputs.text;
	data: simile_timeline_generator_outputs.html
)
text_cleaner_2(text: text_cleaner_outputs.text)
stream_delimiter_filter(object: tuple_to_entity_xml_outputs.xml)
concatenate_text(
	text: htrc_page_retriever_outputs.volume_id;
	text2: htrc_page_retriever_outputs.page_id
)
date_filter(
	min_value: entity_xml_to_simile_xml_outputs.min_value;
	max_value: entity_xml_to_simile_xml_outputs.max_value
)
strings_to_java_string(text: tuple_value_to_string_outputs.text)
read_text(location: flow_parameter_outputs.text)
tuple_to_entity_xml(
	tuples: opennlp_named_entity_outputs.tuples;
	meta_tuple: opennlp_named_entity_outputs.meta_tuple;
	tokens: fork_x2_outputs.object2;
	location: concatenate_text_outputs.text
)
opennlp_sentence_tokenizer(sentences: opennlp_sentence_detector_outputs.sentences)
htrc_page_retriever(volume_id_list: search_text_outputs.text_found)
tuple_value_to_string(
	meta_tuple: csv_to_tuple_outputs.meta_tuple;
	tuples: csv_to_tuple_outputs.tuples
)
csv_to_tuple(text: read_text_outputs.text)
text_cleaner_4(text: random_sample_outputs.text)
simile_timeline_generator(
	xml: xml_to_xml_with_xsl_2_outputs.xml_or_text;
	max_value: date_filter_outputs.max_value;
	min_value: date_filter_outputs.min_value
)
search_text(text: text_cleaner_4_outputs.text)
text_cleaner_3(text: text_cleaner_2_outputs.text)

