#
# Generated by RDF2ZZConverter v1.4.11.8693M on Mon Mar 23 10:03:06 CDT 2015
#
# @name 	HTRC_Spellcheck_Report_Per_Volume
# @description 	<br>
# @creator 	admin
# @date 	Mon Mar 23 10:02:32 CDT 2015
# @rights 	UIUC/NCSA Open Source License
# @tags 	spellcheck, htrc
# @uri  	meandre://htrc.illinois.edu/flows/htrc_spellcheck_report_per_volume/
#

#
# Specify component imports
#
# TODO: Add component import statement(s) here
# Example: import <URL>   (replace 'URL' with the correct location)
import <http://dev5.informatics.illinois.edu:1714/public/services/repository.ttl>
#
# Create the component aliases
#
alias <meandre://seasr.org/components/foundry/csv-to-tuple> as CSV_TO_TUPLE
alias <meandre://seasr.org/components/foundry/token-counts-to-csv> as TOKEN_COUNTS_TO_CSV
alias <meandre://seasr.org/components/foundry/add-tuple-attribute> as ADD_TUPLE_ATTRIBUTE
alias <meandre://seasr.org/components/foundry/tuple-to-html> as TUPLE_TO_HTML
alias <meandre://seasr.org/components/foundry/write-to-file> as WRITE_TO_FILE
alias <meandre://seasr.org/components/foundry/text-cleaner> as TEXT_CLEANER
alias <meandre://seasr.org/components/foundry/read-text> as READ_TEXT
alias <meandre://seasr.org/components/foundry/to-lowercase> as TO_LOWERCASE
alias <meandre://seasr.org/components/foundry/spell-check-with-counts> as SPELL_CHECK_WITH_COUNTS
alias <meandre://seasr.org/components/foundry/csv-to-token-counts> as CSV_TO_TOKEN_COUNTS
alias <meandre://seasr.org/components/foundry/universal-text-extractor> as UNIVERSAL_TEXT_EXTRACTOR
alias <meandre://seasr.org/components/foundry/opennlp-tokenizer> as OPENNLP_TOKENIZER
alias <meandre://seasr.org/components/foundry/tuple-value-to-string> as TUPLE_VALUE_TO_STRING
alias <meandre://seasr.org/components/foundry/token-counter-reducer> as TOKEN_COUNTER_REDUCER
alias <meandre://seasr.org/components/foundry/token-counter> as TOKEN_COUNTER
alias <meandre://seasr.org/components/foundry/search-text> as SEARCH_TEXT
alias <meandre://seasr.org/components/foundry/tokens-to-text> as TOKENS_TO_TEXT
alias <meandre://seasr.org/components/foundry/strings-to-java-string> as STRINGS_TO_JAVA_STRING
alias <meandre://seasr.org/components/foundry/text-accumulator> as TEXT_ACCUMULATOR
alias <meandre://seasr.org/components/foundry/push-text> as PUSH_TEXT
alias <meandre://seasr.org/components/foundry/flow-parameter> as FLOW_PARAMETER
alias <meandre://seasr.org/components/foundry/random-sample> as RANDOM_SAMPLE
alias <meandre://seasr.org/components/htrc/htrc-page-retriever> as HTRC_PAGE_RETRIEVER
alias <meandre://seasr.org/components/foundry/tuple-aggregator> as TUPLE_AGGREGATOR
alias <meandre://seasr.org/components/foundry/stream-delimiter-filter> as STREAM_DELIMITER_FILTER

#
# Create the component instances
#
text_accumulator_2 = TEXT_ACCUMULATOR()
write_to_file_3 = WRITE_TO_FILE()
strings_to_java_string = STRINGS_TO_JAVA_STRING()
write_to_file_2 = WRITE_TO_FILE()
text_cleaner = TEXT_CLEANER()
push_text_3 = PUSH_TEXT()
push_text = PUSH_TEXT()
stream_delimiter_filter_5 = STREAM_DELIMITER_FILTER()
htrc_page_retriever = HTRC_PAGE_RETRIEVER()
read_text = READ_TEXT()
push_text_4 = PUSH_TEXT()
stream_delimiter_filter_2 = STREAM_DELIMITER_FILTER()
write_to_file = WRITE_TO_FILE()
search_text = SEARCH_TEXT()
stream_delimiter_filter = STREAM_DELIMITER_FILTER()
stream_delimiter_filter_4 = STREAM_DELIMITER_FILTER()
opennlp_tokenizer = OPENNLP_TOKENIZER()
text_accumulator_3 = TEXT_ACCUMULATOR()
flow_parameter = FLOW_PARAMETER()
to_lowercase = TO_LOWERCASE()
tokens_to_text = TOKENS_TO_TEXT()
add_tuple_attribute = ADD_TUPLE_ATTRIBUTE()
flow_parameter_2 = FLOW_PARAMETER()
text_accumulator = TEXT_ACCUMULATOR()
random_sample = RANDOM_SAMPLE()
token_counter_reducer = TOKEN_COUNTER_REDUCER()
tuple_to_html = TUPLE_TO_HTML()
tuple_value_to_string = TUPLE_VALUE_TO_STRING()
spell_check_with_counts = SPELL_CHECK_WITH_COUNTS()
csv_to_tuple = CSV_TO_TUPLE()
text_cleaner_2 = TEXT_CLEANER()
push_text_2 = PUSH_TEXT()
csv_to_token_counts = CSV_TO_TOKEN_COUNTS()
token_counts_to_csv = TOKEN_COUNTS_TO_CSV()
search_text_2 = SEARCH_TEXT()
token_counter_reducer_2 = TOKEN_COUNTER_REDUCER()
flow_parameter_4 = FLOW_PARAMETER()
token_counter = TOKEN_COUNTER()
write_to_file_4 = WRITE_TO_FILE()
flow_parameter_3 = FLOW_PARAMETER()
tuple_aggregator = TUPLE_AGGREGATOR()
universal_text_extractor = UNIVERSAL_TEXT_EXTRACTOR()

#
# Set component properties
#
text_accumulator_2._debug_level = "info"
text_accumulator_2.separator = "\\n"
text_accumulator_2._stream_id = "3"
text_accumulator_2._ignore_errors = "false"

write_to_file_3._debug_level = "info"
write_to_file_3.append_timestamp = "false"
write_to_file_3.append_data_to_file = "false"
write_to_file_3._ignore_errors = "false"
write_to_file_3.default_folder = "/tmp"

strings_to_java_string._debug_level = "info"
strings_to_java_string.separator = "\\n"
strings_to_java_string._ignore_errors = "false"

write_to_file_2._debug_level = "info"
write_to_file_2.append_timestamp = "false"
write_to_file_2.append_data_to_file = "false"
write_to_file_2.default_folder = "/tmp"
write_to_file_2._ignore_errors = "false"

text_cleaner.replace4 = ""
text_cleaner.replace3 = ""
text_cleaner.replace2 = ""
text_cleaner.replace = ""
text_cleaner._ignore_errors = "false"
text_cleaner.find = "^(.*)\\s"
text_cleaner.find4 = "\\b[0-9,.]*[0-9]\\b"
text_cleaner.find3 = "(?m)--?\\s*$\\s*"
text_cleaner.find2 = "(.*)$"
text_cleaner._debug_level = "info"

push_text_3._debug_level = "info"
push_text_3.message = "misspellings_with_counts.txt"
push_text_3._ignore_errors = "false"

push_text._debug_level = "info"
push_text.message = "misspellings.txt"
push_text._ignore_errors = "false"

stream_delimiter_filter_5.advanced_filter = ""
stream_delimiter_filter_5._debug_level = "info"
stream_delimiter_filter_5._stream_id = ""
stream_delimiter_filter_5._ignore_errors = "false"

htrc_page_retriever.delimiter = "|"
htrc_page_retriever.wrap_stream = "true"
htrc_page_retriever.read_timeout = "0"
htrc_page_retriever.stream_per_volume = "true"
htrc_page_retriever._ignore_errors = "false"
htrc_page_retriever.auth_selfsign = "false"
htrc_page_retriever.auth_token = "a6fa7057bfa0e5a9a33411e41ca9bc69"
htrc_page_retriever._stream_id = "5"
htrc_page_retriever.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever._debug_level = "info"
htrc_page_retriever.connection_timeout = "0"

read_text.retry_on_timeout = "true"
read_text.read_timeout = "0"
read_text.max_attempts = "1"
read_text._ignore_errors = "false"
read_text.retry_on_http_error = "0"
read_text.retry_delay = "1000"
read_text.connection_timeout = "0"
read_text._debug_level = "info"

push_text_4._debug_level = "info"
push_text_4.message = "replacement_rules.txt"
push_text_4._ignore_errors = "false"

stream_delimiter_filter_2.advanced_filter = ""
stream_delimiter_filter_2._debug_level = "info"
stream_delimiter_filter_2._stream_id = ""
stream_delimiter_filter_2._ignore_errors = "false"

write_to_file._debug_level = "info"
write_to_file.append_timestamp = "false"
write_to_file.append_data_to_file = "false"
write_to_file._ignore_errors = "false"
write_to_file.default_folder = "/tmp"

search_text._debug_level = "info"
search_text._stream_id = "3"
search_text.expression = "(?:[^|]+\\|?){1,4}"
search_text.wrap_stream = "true"
search_text._ignore_errors = "false"

stream_delimiter_filter._debug_level = "info"
stream_delimiter_filter.advanced_filter = ""
stream_delimiter_filter._stream_id = ""
stream_delimiter_filter._ignore_errors = "false"

stream_delimiter_filter_4.advanced_filter = ""
stream_delimiter_filter_4._debug_level = "info"
stream_delimiter_filter_4._stream_id = ""
stream_delimiter_filter_4._ignore_errors = "false"

opennlp_tokenizer.tokenizer_type = "simple"
opennlp_tokenizer._debug_level = "info"
opennlp_tokenizer._ignore_errors = "false"

text_accumulator_3._debug_level = "info"
text_accumulator_3.separator = "\\n"
text_accumulator_3._stream_id = "3"
text_accumulator_3._ignore_errors = "false"

flow_parameter.default_value = "/home/lauvil/HTRC/data/volume_id.txt"
flow_parameter._debug_level = "info"
flow_parameter.param_name = "volume_id"
flow_parameter._stream_id = "99"
flow_parameter.wrap_stream = "false"
flow_parameter._ignore_errors = "false"

to_lowercase._debug_level = "info"
to_lowercase._ignore_errors = "false"

tokens_to_text.encoding = "UTF-8"
tokens_to_text._ignore_errors = "false"
tokens_to_text.offset = "0"
tokens_to_text.header = ""
tokens_to_text.separator = ","
tokens_to_text.count = "-1"
tokens_to_text.message = "Tokens"
tokens_to_text._debug_level = "info"

add_tuple_attribute._debug_level = "info"
add_tuple_attribute.attribute_name = "volume_id"
add_tuple_attribute._ignore_errors = "false"

flow_parameter_2.default_value = "http://repository.seasr.org/Datasets/Text/ngram_counts.txt"
flow_parameter_2._debug_level = "info"
flow_parameter_2.param_name = "token_counts"
flow_parameter_2._stream_id = "99"
flow_parameter_2._ignore_errors = "false"
flow_parameter_2.wrap_stream = "false"

text_accumulator._debug_level = "info"
text_accumulator.separator = " "
text_accumulator._stream_id = "5"
text_accumulator._ignore_errors = "false"

random_sample.seed = "123"
random_sample._debug_level = "info"
random_sample.count = "10"
random_sample._ignore_errors = "false"

token_counter_reducer.ordered = "true"
token_counter_reducer._debug_level = "info"
token_counter_reducer._stream_id = "5"
token_counter_reducer._ignore_errors = "false"

tuple_to_html._debug_level = "info"
tuple_to_html.properties = ""
tuple_to_html.css = ""
tuple_to_html.template = "org/seasr/meandre/components/tools/tuples/TupleToHTML.vm"
tuple_to_html._ignore_errors = "false"

tuple_value_to_string._debug_level = "info"
tuple_value_to_string.attribute = "volume_id"
tuple_value_to_string._ignore_errors = "false"

spell_check_with_counts.ignore_digitwords = "false"
spell_check_with_counts.ignore_uppercase = "false"
spell_check_with_counts.ignore_internetaddresses = "true"
spell_check_with_counts.do_correction = "false"
spell_check_with_counts._ignore_errors = "false"
spell_check_with_counts.min_rule_support = "1"
spell_check_with_counts.ignore_mixedcase = "false"
spell_check_with_counts.enable_levenshtein = "true"
spell_check_with_counts.levenshtein_distance = ".20"
spell_check_with_counts.transform_threshold = "25"
spell_check_with_counts.output_misspellings_with_counts = "true"
spell_check_with_counts.enable_transforms_only = "true"
spell_check_with_counts._debug_level = "info"

csv_to_tuple._debug_level = "info"
csv_to_tuple.header = "false"
csv_to_tuple.labels = "volume_id"
csv_to_tuple._ignore_errors = "false"

text_cleaner_2.replace4 = ""
text_cleaner_2.replace3 = ""
text_cleaner_2.replace2 = ""
text_cleaner_2.replace = "|"
text_cleaner_2._ignore_errors = "false"
text_cleaner_2.find = "\\n"
text_cleaner_2.find4 = ""
text_cleaner_2.find3 = ""
text_cleaner_2.find2 = ""
text_cleaner_2._debug_level = "info"

push_text_2._debug_level = "info"
push_text_2.message = "spellcheck_report.html"
push_text_2._ignore_errors = "false"

csv_to_token_counts.enable_constraint_checking = "false"
csv_to_token_counts.column_count = ""
csv_to_token_counts._ignore_errors = "false"
csv_to_token_counts.header = "true"
csv_to_token_counts.ordered = "false"
csv_to_token_counts.token_pos = "0"
csv_to_token_counts.count_pos = "1"
csv_to_token_counts._debug_level = "info"

token_counts_to_csv.ordered = "true"
token_counts_to_csv._debug_level = "info"
token_counts_to_csv.header = "tokens,counts"
token_counts_to_csv._ignore_errors = "false"

search_text_2._debug_level = "info"
search_text_2._stream_id = "0"
search_text_2.expression = "^\\S*"
search_text_2._ignore_errors = "false"
search_text_2.wrap_stream = "false"

token_counter_reducer_2.ordered = "true"
token_counter_reducer_2._debug_level = "info"
token_counter_reducer_2._stream_id = "3"
token_counter_reducer_2._ignore_errors = "false"

flow_parameter_4.default_value = "http://repository.seasr.org/Datasets/Text/dict"
flow_parameter_4._debug_level = "info"
flow_parameter_4.param_name = "dictionary"
flow_parameter_4._stream_id = "99"
flow_parameter_4.wrap_stream = "false"
flow_parameter_4._ignore_errors = "false"

token_counter.ordered = "false"
token_counter._debug_level = "info"
token_counter._ignore_errors = "false"

write_to_file_4._debug_level = "info"
write_to_file_4.append_timestamp = "false"
write_to_file_4.append_data_to_file = "false"
write_to_file_4.default_folder = "/tmp"
write_to_file_4._ignore_errors = "false"

flow_parameter_3.default_value = "o=0; i=1; l=1; z=2; o=3; e=3; s=3; d=3; t=4;e=4; l=4; s=o; s=5; c=6; e=6; fi=6; o=6; l=7; z=7; y=7; j=8; g=8; s=8; a=9; c=9; g=9; o=9; ti=9; b={h,o}; c={e,o,q}; cl={ct,d}; ct={cl,d,dl,dt,ft}; d={cl,ct}; dl=ct; dt=ct; e=c; fl={ss,st}; ft=ct; h={li,b,ii,ll}; i={l,r,t}; in=m; j=y; l={i,t}; li=h; m={rn,lll,iii,in,ni}; n={ll,il,ii,h}; ni=m; oe=ce; r=ll; rn=m; s=f; sh={fli,ih,jb,jh,m,sb}; ss=fl; st=fl; t=l; tb=th; th=tb; v=y; u={ll,n,ti,ii}; y={j,v};"
flow_parameter_3._debug_level = "info"
flow_parameter_3.param_name = "transformation_rules"
flow_parameter_3._stream_id = "99"
flow_parameter_3.wrap_stream = "false"
flow_parameter_3._ignore_errors = "false"

tuple_aggregator._debug_level = "info"
tuple_aggregator._stream_id = "3"
tuple_aggregator._ignore_errors = "false"

universal_text_extractor._debug_level = "info"
universal_text_extractor.read_timeout = "0"
universal_text_extractor.connection_timeout = "0"
universal_text_extractor._ignore_errors = "false"

#
# Create the flow by connecting the components
#
@flow_parameter_3_outputs = flow_parameter_3()
@flow_parameter_4_outputs = flow_parameter_4()
@flow_parameter_2_outputs = flow_parameter_2()
@token_counter_reducer_2_outputs = token_counter_reducer_2()
@random_sample_outputs = random_sample()
@csv_to_token_counts_outputs = csv_to_token_counts()
@spell_check_with_counts_outputs = spell_check_with_counts()
@stream_delimiter_filter_outputs = stream_delimiter_filter()
@strings_to_java_string_outputs = strings_to_java_string()
@to_lowercase_outputs = to_lowercase()
@read_text_outputs = read_text()
@token_counter_reducer_outputs = token_counter_reducer()
@htrc_page_retriever_outputs = htrc_page_retriever()
@tuple_value_to_string_outputs = tuple_value_to_string()
@text_accumulator_outputs = text_accumulator()
@opennlp_tokenizer_outputs = opennlp_tokenizer() [+AUTO!]
@csv_to_tuple_outputs = csv_to_tuple()
@push_text_outputs = push_text()
@add_tuple_attribute_outputs = add_tuple_attribute()
@tokens_to_text_outputs = tokens_to_text()
@stream_delimiter_filter_4_outputs = stream_delimiter_filter_4()
@stream_delimiter_filter_5_outputs = stream_delimiter_filter_5()
@flow_parameter_outputs = flow_parameter()
@push_text_2_outputs = push_text_2()
@token_counts_to_csv_outputs = token_counts_to_csv()
@stream_delimiter_filter_2_outputs = stream_delimiter_filter_2()
@push_text_3_outputs = push_text_3()
@push_text_4_outputs = push_text_4()
@tuple_to_html_outputs = tuple_to_html()
@text_cleaner_outputs = text_cleaner() [+AUTO!]
@text_cleaner_2_outputs = text_cleaner_2()
@token_counter_outputs = token_counter() [+AUTO!]
@tuple_aggregator_outputs = tuple_aggregator()
@text_accumulator_3_outputs = text_accumulator_3()
@text_accumulator_2_outputs = text_accumulator_2()
@universal_text_extractor_outputs = universal_text_extractor()
@search_text_2_outputs = search_text_2()
@search_text_outputs = search_text()

write_to_file_4(
	data: stream_delimiter_filter_2_outputs.object;
	location: push_text_2_outputs.text
)
write_to_file_3(
	location: push_text_3_outputs.text;
	data: stream_delimiter_filter_5_outputs.object
)
write_to_file_2(
	data: stream_delimiter_filter_outputs.object;
	location: push_text_outputs.text
)
random_sample(text: strings_to_java_string_outputs.java_string)
token_counter_reducer_2(token_counts: spell_check_with_counts_outputs.misspellings_with_counts)
csv_to_token_counts(text: universal_text_extractor_outputs.text)
spell_check_with_counts(
	text: token_counter_reducer_outputs.token_counts;
	transformations: flow_parameter_3_outputs.text;
	token_counts: csv_to_token_counts_outputs.token_counts;
	dictionary: flow_parameter_4_outputs.text
)
stream_delimiter_filter(object: text_accumulator_3_outputs.text)
to_lowercase(text: text_cleaner_outputs.text)
strings_to_java_string(text: tuple_value_to_string_outputs.text)
read_text(location: flow_parameter_outputs.text)
token_counter_reducer(token_counts: token_counter_outputs.token_counts)
htrc_page_retriever(volume_id_list: search_text_outputs.text_found)
tuple_value_to_string(
	tuples: csv_to_tuple_outputs.tuples;
	meta_tuple: csv_to_tuple_outputs.meta_tuple
)
text_accumulator(text: htrc_page_retriever_outputs.volume_id)
opennlp_tokenizer(text: to_lowercase_outputs.text)
csv_to_tuple(text: read_text_outputs.text)
add_tuple_attribute(
	tuples: spell_check_with_counts_outputs.tuples;
	meta_tuple: spell_check_with_counts_outputs.meta_tuple;
	attribute: search_text_2_outputs.text_found
)
tokens_to_text(tokens: spell_check_with_counts_outputs.uncorrected_misspellings)
stream_delimiter_filter_4(object: text_accumulator_2_outputs.text)
stream_delimiter_filter_5(object: token_counts_to_csv_outputs.text)
token_counts_to_csv(token_counts: token_counter_reducer_2_outputs.token_counts)
stream_delimiter_filter_2(object: tuple_to_html_outputs.html)
tuple_to_html(
	tuples: tuple_aggregator_outputs.tuples;
	meta_tuple: tuple_aggregator_outputs.meta_tuple
)
text_cleaner(text: htrc_page_retriever_outputs.text)
write_to_file(
	data: stream_delimiter_filter_4_outputs.object;
	location: push_text_4_outputs.text
)
text_cleaner_2(text: random_sample_outputs.text)
tuple_aggregator(
	tuples: add_tuple_attribute_outputs.tuples;
	meta_tuple: add_tuple_attribute_outputs.meta_tuple
)
token_counter(tokens: opennlp_tokenizer_outputs.tokens)
text_accumulator_3(text: tokens_to_text_outputs.text)
text_accumulator_2(text: spell_check_with_counts_outputs.replacement_rules)
universal_text_extractor(location: flow_parameter_2_outputs.text)
search_text_2(text: text_accumulator_outputs.text)
search_text(text: text_cleaner_2_outputs.text)

