#
# Generated by RDF2ZZConverter v1.4.9.8618 on Tue Aug 27 19:45:57 CDT 2013
#
# @name 	HTRC_Topic_Modeling
# @description 	<br>
# @creator 	admin
# @date 	Tue Aug 27 19:45:40 CDT 2013
# @rights 	UIUC/NCSA Open Source License
# @tags 	topic modeling, htrc
# @uri  	meandre://htrc.illinois.edu/flows/htrc_topic_modeling/
#

#
# Specify component imports
#
# TODO: Add component import statement(s) here
# Example: import <URL>   (replace 'URL' with the correct location)
import <http://repository.seasr.org/Meandre/Locations/Latest/Components/repository_components.nt>
import <http://repository.seasr.org/Meandre/Locations/Latest/HTRC_Components/repository_components.nt>

#
# Create the component aliases
#
alias <meandre://seasr.org/components/foundry/tuple-value-to-string> as TUPLE_VALUE_TO_STRING
alias <meandre://seasr.org/components/foundry/write-to-file> as WRITE_TO_FILE
alias <meandre://seasr.org/components/foundry/random-sample> as RANDOM_SAMPLE
alias <meandre://seasr.org/components/foundry/concatenate-text> as CONCATENATE_TEXT
alias <meandre://seasr.org/components/foundry/search-text> as SEARCH_TEXT
alias <meandre://seasr.org/components/foundry/strings-to-java-string> as STRINGS_TO_JAVA_STRING
alias <meandre://seasr.org/components/foundry/tuple-value-filter> as TUPLE_VALUE_FILTER
alias <meandre://seasr.org/components/foundry/tag-cloud> as TAG_CLOUD
alias <meandre://seasr.org/components/foundry/text-cleaner> as TEXT_CLEANER
alias <meandre://seasr.org/components/foundry/select-nodes-via-xpath> as SELECT_NODES_VIA_XPATH
alias <meandre://seasr.org/components/foundry/xml-to-xml-with-xsl> as XML_TO_XML_WITH_XSL
alias <meandre://seasr.org/components/htrc/htrc-page-retriever> as HTRC_PAGE_RETRIEVER
alias <meandre://seasr.org/components/foundry/read-text> as READ_TEXT
alias <meandre://seasr.org/components/foundry/opennlp-sentence-detector> as OPENNLP_SENTENCE_DETECTOR
alias <meandre://seasr.org/components/foundry/opennlp-pos-tagger> as OPENNLP_POS_TAGGER
alias <meandre://seasr.org/components/foundry/topic-top-words-to-xml> as TOPIC_TOP_WORDS_TO_XML
alias <meandre://seasr.org/components/foundry/push-text> as PUSH_TEXT
alias <meandre://seasr.org/components/foundry/flow-parameter> as FLOW_PARAMETER
alias <meandre://seasr.org/components/foundry/csv-to-tuple> as CSV_TO_TUPLE
alias <meandre://seasr.org/components/foundry/fork-x5> as FORK_X5
alias <meandre://seasr.org/components/foundry/frame-maker> as FRAME_MAKER
alias <meandre://seasr.org/components/foundry/tuple-value-frequency-counter> as TUPLE_VALUE_FREQUENCY_COUNTER
alias <meandre://seasr.org/components/foundry/csv-to-token-counts> as CSV_TO_TOKEN_COUNTS
alias <meandre://seasr.org/components/foundry/aggregate-mallet-instances> as AGGREGATE_MALLET_INSTANCES
alias <meandre://seasr.org/components/foundry/create-mallet-instance> as CREATE_MALLET_INSTANCE
alias <meandre://seasr.org/components/foundry/tuple-to-mallet-feature-sequence> as TUPLE_TO_MALLET_FEATURE_SEQUENCE
alias <meandre://seasr.org/components/foundry/opennlp-sentence-tokenizer> as OPENNLP_SENTENCE_TOKENIZER
alias <meandre://seasr.org/components/foundry/top-n-filter> as TOP_N_FILTER
alias <meandre://seasr.org/components/foundry/mallet-topic-modeling> as MALLET_TOPIC_MODELING
alias <meandre://seasr.org/components/foundry/stream-delimiter-filter> as STREAM_DELIMITER_FILTER

#
# Create the component instances
#
stream_delimiter_filter_2 = STREAM_DELIMITER_FILTER()
opennlp_sentence_detector = OPENNLP_SENTENCE_DETECTOR()
tag_cloud = TAG_CLOUD()
random_sample = RANDOM_SAMPLE()
xml_to_xml_with_xsl = XML_TO_XML_WITH_XSL()
strings_to_java_string = STRINGS_TO_JAVA_STRING()
text_cleaner_3 = TEXT_CLEANER()
text_cleaner = TEXT_CLEANER()
stream_delimiter_filter = STREAM_DELIMITER_FILTER()
frame_maker = FRAME_MAKER()
concatenate_text = CONCATENATE_TEXT()
search_text = SEARCH_TEXT()
create_mallet_instance = CREATE_MALLET_INSTANCE()
agg_mallet_inst = AGGREGATE_MALLET_INSTANCES()
tuple_value_filter = TUPLE_VALUE_FILTER()
select_nodes_via_xpath = SELECT_NODES_VIA_XPATH()
opennlp_sentence_tokenizer = OPENNLP_SENTENCE_TOKENIZER()
csv_to_tuple = CSV_TO_TUPLE()
write_to_file = WRITE_TO_FILE()
push_text = PUSH_TEXT()
to_mallet_feat_seq = TUPLE_TO_MALLET_FEATURE_SEQUENCE()
topic_top_words_to_xml = TOPIC_TOP_WORDS_TO_XML()
push_text_3 = PUSH_TEXT()
tuple_value_frequency_counter = TUPLE_VALUE_FREQUENCY_COUNTER()
csv_to_token_counts = CSV_TO_TOKEN_COUNTS()
read_text = READ_TEXT()
htrc_page_retriever = HTRC_PAGE_RETRIEVER()
top_n_filter = TOP_N_FILTER()
read_text_2 = READ_TEXT()
mallet_topic_modeling = MALLET_TOPIC_MODELING()
flow_parameter = FLOW_PARAMETER()
text_cleaner_2 = TEXT_CLEANER()
write_to_file_2 = WRITE_TO_FILE()
push_text_2 = PUSH_TEXT()
tuple_value_to_string = TUPLE_VALUE_TO_STRING()
opennlp_pos_tagger = OPENNLP_POS_TAGGER()
fork_x5 = FORK_X5()

#
# Set component properties
#
stream_delimiter_filter_2.advanced_filter = ""
stream_delimiter_filter_2._debug_level = "info"
stream_delimiter_filter_2._stream_id = ""
stream_delimiter_filter_2._ignore_errors = "false"

opennlp_sentence_detector._debug_level = "info"
opennlp_sentence_detector.lang_code = "en"
opennlp_sentence_detector.remove_newline = "false"
opennlp_sentence_detector._ignore_errors = "false"

tag_cloud.show_tooltip = "true"
tag_cloud.d3_api_url = "http://d3js.org/d3.v2.min.js"
tag_cloud.max_size = "100"
tag_cloud.show_counts = "false"
tag_cloud.force_positive = "true"
tag_cloud._ignore_errors = "false"
tag_cloud.d3_cloud_api_url = "http://www.jasondavies.com/wordcloud/d3.layout.cloud.js"
tag_cloud.properties = "rotation=1"
tag_cloud.css = ""
tag_cloud._debug_level = "info"
tag_cloud.height = "300"
tag_cloud.width = "300"
tag_cloud.font_name = ""
tag_cloud.color_palette = "category20"
tag_cloud.min_size = "10"
tag_cloud.template = "org/seasr/meandre/components/vis/d3/TagCloud.vm"
tag_cloud.scale = "linear"
tag_cloud.title = "Tag Cloud"

random_sample.seed = "123"
random_sample._debug_level = "info"
random_sample.count = "10"
random_sample._ignore_errors = "false"

xml_to_xml_with_xsl._debug_level = "info"
xml_to_xml_with_xsl._ignore_errors = "false"

strings_to_java_string._debug_level = "info"
strings_to_java_string.separator = "\\n"
strings_to_java_string._ignore_errors = "false"

text_cleaner_3.replace4 = ""
text_cleaner_3.replace3 = ""
text_cleaner_3.replace2 = ""
text_cleaner_3.replace = "|"
text_cleaner_3._ignore_errors = "false"
text_cleaner_3.find = "\\n"
text_cleaner_3.find4 = ""
text_cleaner_3.find3 = ""
text_cleaner_3.find2 = ""
text_cleaner_3._debug_level = "info"

text_cleaner.replace4 = ""
text_cleaner.replace3 = ""
text_cleaner.replace2 = ""
text_cleaner.replace = ""
text_cleaner._ignore_errors = "false"
text_cleaner.find = "^(.*)\\s"
text_cleaner.find4 = "[^\\p{L}\\p{Z}\\p{S}\\p{N}\\p{P}]"
text_cleaner.find3 = "(?m)--?\\s*$\\s*"
text_cleaner.find2 = "(.*)$"
text_cleaner._debug_level = "info"

stream_delimiter_filter.advanced_filter = ""
stream_delimiter_filter._debug_level = "info"
stream_delimiter_filter._stream_id = ""
stream_delimiter_filter._ignore_errors = "false"

frame_maker.columns = "300"
frame_maker._ignore_errors = "false"
frame_maker.frame_ref_url = ""
frame_maker.output_folder = "/tmp/frames"
frame_maker._stream_id = "5"
frame_maker.clean_up = "false"
frame_maker.template = "org/seasr/meandre/components/vis/html/iFrameViewer.vm"
frame_maker._debug_level = "info"

concatenate_text._debug_level = "info"
concatenate_text.separator = ":"
concatenate_text._ignore_errors = "false"

search_text._debug_level = "info"
search_text._stream_id = "1"
search_text.expression = "(?:[^|]+\\|?){1,4}"
search_text._ignore_errors = "false"
search_text.wrap_stream = "true"

create_mallet_instance._debug_level = "info"
create_mallet_instance._ignore_errors = "false"

agg_mallet_inst._debug_level = "info"
agg_mallet_inst._stream_id = "1"
agg_mallet_inst._ignore_errors = "false"

tuple_value_filter.filter_attribute = "token"
tuple_value_filter._debug_level = "severe"
tuple_value_filter.filter_regex = "(?i)^a|a's|able|about|above|according|accordingly|across|actually|after|afterwards|again|against|ain't|all|allow|allows|almost|alone|along|already|also|although|always|am|among|amongst|an|and|another|any|anybody|anyhow|anyone|anything|anyway|anyways|anywhere|apart|appear|appreciate|appropriate|are|aren't|around|as|aside|ask|asking|associated|at|available|away|awfully|b|be|became|because|become|becomes|becoming|been|before|beforehand|behind|being|believe|below|beside|besides|best|better|between|beyond|both|brief|but|by|c|c'mon|c's|came|can|can't|cannot|cant|cause|causes|certain|certainly|changes|clearly|co|com|come|comes|concerning|consequently|consider|considering|contain|containing|contains|corresponding|could|couldn't|course|currently|d|definitely|described|despite|did|didn't|different|do|does|doesn't|doing|don't|done|down|downwards|during|e|each|edu|eg|eight|either|else|elsewhere|enough|entirely|especially|et|etc|even|ever|every|everybody|everyone|everything|everywhere|ex|exactly|example|except|f|far|few|fifth|first|five|followed|following|follows|for|former|formerly|forth|four|from|further|furthermore|g|get|gets|getting|given|gives|go|goes|going|gone|got|gotten|greetings|h|had|hadn't|happens|hardly|has|hasn't|have|haven't|having|he|he's|hello|help|hence|her|here|here's|hereafter|hereby|herein|hereupon|hers|herself|hi|him|himself|his|hither|hopefully|how|howbeit|however|i|i'd|i'll|i'm|i've|ie|if|ignored|immediate|in|inasmuch|inc|indeed|indicate|indicated|indicates|inner|insofar|instead|into|inward|is|isn't|it|it'd|it'll|it's|its|itself|j|just|k|keep|keeps|kept|know|knows|known|l|last|lately|later|latter|latterly|least|less|lest|let|let's|like|liked|likely|little|look|looking|looks|ltd|m|mainly|many|may|maybe|me|mean|meanwhile|merely|might|more|moreover|most|mostly|much|must|my|myself|n|name|namely|nd|near|nearly|necessary|need|needs|neither|never|nevertheless|new|next|nine|no|nobody|non|none|noone|nor|normally|not|nothing|novel|now|nowhere|o|obviously|of|off|often|oh|ok|okay|old|on|once|one|ones|only|onto|or|other|others|otherwise|ought|our|ours|ourselves|out|outside|over|overall|own|p|particular|particularly|per|perhaps|placed|please|plus|possible|presumably|probably|provides|q|que|quite|qv|r|rather|rd|re|really|reasonably|regarding|regardless|regards|relatively|respectively|right|s|said|same|saw|say|saying|says|second|secondly|see|seeing|seem|seemed|seeming|seems|seen|self|selves|sensible|sent|serious|seriously|seven|several|shall|she|should|shouldn't|since|six|so|some|somebody|somehow|someone|something|sometime|sometimes|somewhat|somewhere|soon|sorry|specified|specify|specifying|still|sub|such|sup|sure|t|t's|take|taken|tell|tends|th|than|thank|thanks|thanx|that|that's|thats|the|their|theirs|them|themselves|then|thence|there|there's|thereafter|thereby|therefore|therein|theres|thereupon|these|they|they'd|they'll|they're|they've|think|third|this|thorough|thoroughly|those|though|three|through|throughout|thru|thus|to|together|too|took|toward|towards|tried|tries|truly|try|trying|twice|two|u|un|under|unfortunately|unless|unlikely|until|unto|up|upon|us|use|used|useful|uses|using|usually|uucp|v|value|various|very|via|viz|vs|w|want|wants|was|wasn't|way|we|we'd|we'll|we're|we've|welcome|well|went|were|weren't|what|what's|whatever|when|whence|whenever|where|where's|whereafter|whereas|whereby|wherein|whereupon|wherever|whether|which|while|whither|who|who's|whoever|whole|whom|whose|why|will|willing|wish|with|within|without|won't|wonder|would|would|wouldn't|x|y|yes|yet|you|you'd|you'll|you're|you've|your|yours|yourself|yourselves|z|zero$"
tuple_value_filter.filter_out = "true"
tuple_value_filter._ignore_errors = "false"

select_nodes_via_xpath.wrap_stream = "true"
select_nodes_via_xpath.namespaces = ""
select_nodes_via_xpath.qname = "NODESET"
select_nodes_via_xpath._ignore_errors = "false"
select_nodes_via_xpath.xpath = "//topic"
select_nodes_via_xpath._stream_id = "5"
select_nodes_via_xpath._debug_level = "info"

opennlp_sentence_tokenizer._debug_level = "info"
opennlp_sentence_tokenizer.lang_code = "en"
opennlp_sentence_tokenizer._ignore_errors = "false"

csv_to_tuple._debug_level = "info"
csv_to_tuple.header = "false"
csv_to_tuple._ignore_errors = "false"
csv_to_tuple.labels = "volume_id"

write_to_file._debug_level = "info"
write_to_file.append_timestamp = "false"
write_to_file.append_data_to_file = "false"
write_to_file.default_folder = "/tmp"
write_to_file._ignore_errors = "false"

push_text._debug_level = "info"
push_text.message = "http://repository.seasr.org/Datasets/xslt/mallet/topicTopWordsToCSV.xslt"
push_text._ignore_errors = "false"

to_mallet_feat_seq._debug_level = "info"
to_mallet_feat_seq.feature_counts_attribute = "count"
to_mallet_feat_seq._stream_id = "1"
to_mallet_feat_seq.feature_attribute = "token"
to_mallet_feat_seq._ignore_errors = "false"

topic_top_words_to_xml._debug_level = "info"
topic_top_words_to_xml.num_top_words = "20"
topic_top_words_to_xml._ignore_errors = "false"

push_text_3._debug_level = "info"
push_text_3.message = "topic_tagclouds.html"
push_text_3._ignore_errors = "false"

tuple_value_frequency_counter.tupleField = "token"
tuple_value_frequency_counter._ignore_errors = "false"
tuple_value_frequency_counter.trim_fields = "token"
tuple_value_frequency_counter.threshold = "0"
tuple_value_frequency_counter.max_size = "-1"
tuple_value_frequency_counter.normalize_fields = "token"
tuple_value_frequency_counter._debug_level = "info"

csv_to_token_counts.enable_constraint_checking = "false"
csv_to_token_counts.column_count = ""
csv_to_token_counts._ignore_errors = "false"
csv_to_token_counts.header = "true"
csv_to_token_counts.ordered = "true"
csv_to_token_counts.token_pos = "1"
csv_to_token_counts.count_pos = "2"
csv_to_token_counts._debug_level = "info"

read_text.retry_on_timeout = "true"
read_text.read_timeout = "0"
read_text.max_attempts = "1"
read_text._ignore_errors = "false"
read_text.retry_on_http_error = "0"
read_text.retry_delay = "1000"
read_text.connection_timeout = "0"
read_text._debug_level = "info"

htrc_page_retriever.delimiter = "|"
htrc_page_retriever.wrap_stream = "false"
htrc_page_retriever.read_timeout = "0"
htrc_page_retriever.stream_per_volume = "false"
htrc_page_retriever._ignore_errors = "false"
htrc_page_retriever.auth_selfsign = "false"
htrc_page_retriever.auth_token = "a6fa7057bfa0e5a9a33411e41ca9bc69"
htrc_page_retriever._stream_id = "99"
htrc_page_retriever.data_api_url = "https://sandbox.htrc.illinois.edu:25443/data-api"
htrc_page_retriever.connection_timeout = "0"
htrc_page_retriever._debug_level = "info"

top_n_filter.ordered = "true"
top_n_filter._debug_level = "info"
top_n_filter.bottom_n = "false"
top_n_filter._ignore_errors = "false"
top_n_filter.n_top_tokens = "50"

read_text_2.retry_on_timeout = "true"
read_text_2.read_timeout = "0"
read_text_2.max_attempts = "1"
read_text_2._ignore_errors = "false"
read_text_2.retry_on_http_error = "0"
read_text_2.retry_delay = "1000"
read_text_2.connection_timeout = "0"
read_text_2._debug_level = "info"

mallet_topic_modeling.num_threads = "8"
mallet_topic_modeling.optimize_interval = "0"
mallet_topic_modeling.use_symmetric_alpha = "false"
mallet_topic_modeling.alpha = "50.0"
mallet_topic_modeling.optimize_burnin = "200"
mallet_topic_modeling._ignore_errors = "false"
mallet_topic_modeling.random_seed = "0"
mallet_topic_modeling.num_top_words = "200"
mallet_topic_modeling.num_topics = "10"
mallet_topic_modeling.beta = "0.01"
mallet_topic_modeling.num_iterations = "1000"
mallet_topic_modeling._debug_level = "info"

flow_parameter.default_value = "/home/lauvil/HTRC/data/volume_id.txt"
flow_parameter._debug_level = "info"
flow_parameter.param_name = "volume_id"
flow_parameter._stream_id = "99"
flow_parameter._ignore_errors = "false"
flow_parameter.wrap_stream = "false"

text_cleaner_2.replace4 = "not_"
text_cleaner_2.replace3 = ""
text_cleaner_2.replace2 = ""
text_cleaner_2.replace = " , "
text_cleaner_2._ignore_errors = "false"
text_cleaner_2.find = ","
text_cleaner_2.find4 = "\\bnot \\b"
text_cleaner_2.find3 = ""
text_cleaner_2.find2 = ""
text_cleaner_2._debug_level = "info"

write_to_file_2._debug_level = "info"
write_to_file_2.append_timestamp = "false"
write_to_file_2.append_data_to_file = "false"
write_to_file_2._ignore_errors = "false"
write_to_file_2.default_folder = "/tmp"

push_text_2._debug_level = "info"
push_text_2.message = "topic_top_words.xml"
push_text_2._ignore_errors = "false"

tuple_value_to_string._debug_level = "info"
tuple_value_to_string.attribute = "volume_id"
tuple_value_to_string._ignore_errors = "false"

opennlp_pos_tagger._debug_level = "info"
opennlp_pos_tagger.lang_code = "en"
opennlp_pos_tagger.filter_regex = "NN|NNS|JJ|JJR|JJS|RB|RBR|RP|VB|VBD|VBG|VBN|VBP|VBZ"
opennlp_pos_tagger._ignore_errors = "false"

fork_x5.replication_mode = "0"
fork_x5.replication_method_name = ""
fork_x5._debug_level = "info"
fork_x5._ignore_errors = "false"

#
# Create the flow by connecting the components
#
@tuple_value_frequency_counter_outputs = tuple_value_frequency_counter()
@csv_to_token_counts_outputs = csv_to_token_counts()
@random_sample_outputs = random_sample()
@agg_mallet_inst_outputs = agg_mallet_inst()
@top_n_filter_outputs = top_n_filter()
@tag_cloud_outputs = tag_cloud()
@concatenate_text_outputs = concatenate_text()
@strings_to_java_string_outputs = strings_to_java_string()
@xml_to_xml_with_xsl_outputs = xml_to_xml_with_xsl()
@push_text_2_outputs = push_text_2()
@tuple_value_filter_outputs = tuple_value_filter()
@push_text_3_outputs = push_text_3()
@search_text_outputs = search_text()
@mallet_topic_modeling_outputs = mallet_topic_modeling()
@opennlp_sentence_detector_outputs = opennlp_sentence_detector() [+AUTO!]
@topic_top_words_to_xml_outputs = topic_top_words_to_xml()
@csv_to_tuple_outputs = csv_to_tuple()
@push_text_outputs = push_text()
@tuple_value_to_string_outputs = tuple_value_to_string()
@opennlp_sentence_tokenizer_outputs = opennlp_sentence_tokenizer() [+AUTO!]
@read_text_2_outputs = read_text_2()
@opennlp_pos_tagger_outputs = opennlp_pos_tagger() [+AUTO!]
@select_nodes_via_xpath_outputs = select_nodes_via_xpath()
@flow_parameter_outputs = flow_parameter()
@stream_delimiter_filter_outputs = stream_delimiter_filter()
@create_mallet_instance_outputs = create_mallet_instance()
@read_text_outputs = read_text()
@text_cleaner_3_outputs = text_cleaner_3()
@text_cleaner_outputs = text_cleaner() [+AUTO!]
@stream_delimiter_filter_2_outputs = stream_delimiter_filter_2()
@text_cleaner_2_outputs = text_cleaner_2() [+AUTO!]
@to_mallet_feat_seq_outputs = to_mallet_feat_seq()
@fork_x5_outputs = fork_x5()
@write_to_file_2_outputs = write_to_file_2()
@htrc_page_retriever_outputs = htrc_page_retriever()
@frame_maker_outputs = frame_maker()

tuple_value_frequency_counter(
	tuples: opennlp_pos_tagger_outputs.tuples;
	meta_tuple: opennlp_pos_tagger_outputs.meta_tuple
)
random_sample(text: strings_to_java_string_outputs.java_string)
csv_to_token_counts(text: xml_to_xml_with_xsl_outputs.xml_or_text)
agg_mallet_inst(mallet_instance: create_mallet_instance_outputs.mallet_instance)
top_n_filter(token_counts: csv_to_token_counts_outputs.token_counts)
concatenate_text(
	text: htrc_page_retriever_outputs.volume_id;
	text2: htrc_page_retriever_outputs.page_id
)
tag_cloud(token_counts: top_n_filter_outputs.token_counts)
strings_to_java_string(text: tuple_value_to_string_outputs.text)
xml_to_xml_with_xsl(
	xsl: read_text_2_outputs.text;
	xml: select_nodes_via_xpath_outputs.xml_or_text
)
tuple_value_filter(
	meta_tuple: tuple_value_frequency_counter_outputs.meta_tuple;
	tuples: tuple_value_frequency_counter_outputs.tuples
)
search_text(text: text_cleaner_3_outputs.text)
mallet_topic_modeling(mallet_instance_list: agg_mallet_inst_outputs.mallet_instance_list)
opennlp_sentence_detector(text: text_cleaner_2_outputs.text)
topic_top_words_to_xml(topic_model: stream_delimiter_filter_2_outputs.object)
csv_to_tuple(text: read_text_outputs.text)
tuple_value_to_string(
	meta_tuple: csv_to_tuple_outputs.meta_tuple;
	tuples: csv_to_tuple_outputs.tuples
)
read_text_2(location: push_text_outputs.text)
opennlp_pos_tagger(tokenized_sentences: opennlp_sentence_tokenizer_outputs.tokenized_sentences)
opennlp_sentence_tokenizer(sentences: opennlp_sentence_detector_outputs.sentences)
select_nodes_via_xpath(xml: write_to_file_2_outputs.data)
stream_delimiter_filter(object: frame_maker_outputs.html)
create_mallet_instance(
	source: fork_x5_outputs.object2;
	data: to_mallet_feat_seq_outputs.mallet_feature_sequence;
	name: fork_x5_outputs.object;
	target: fork_x5_outputs.object3
)
read_text(location: flow_parameter_outputs.text)
stream_delimiter_filter_2(object: mallet_topic_modeling_outputs.topic_model)
text_cleaner_3(text: random_sample_outputs.text)
text_cleaner(text: htrc_page_retriever_outputs.text)
text_cleaner_2(text: text_cleaner_outputs.text)
to_mallet_feat_seq(
	meta_tuple: tuple_value_filter_outputs.meta_tuple;
	tuples: tuple_value_filter_outputs.tuples
)
write_to_file(
	location: push_text_3_outputs.text;
	data: stream_delimiter_filter_outputs.object
)
fork_x5(object: concatenate_text_outputs.text)
write_to_file_2(
	location: push_text_2_outputs.text;
	data: topic_top_words_to_xml_outputs.topic_top_words_xml
)
htrc_page_retriever(volume_id_list: search_text_outputs.text_found)
frame_maker(html: tag_cloud_outputs.html)

